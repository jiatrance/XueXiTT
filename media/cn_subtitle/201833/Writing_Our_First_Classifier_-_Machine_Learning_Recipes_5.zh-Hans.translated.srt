1
00:00:00,000 --> 00:00:06,030
 [音乐播放] 

2
00:00:06,030 --> 00:00:06,710
嘿大家。 

3
00:00:06,710 --> 00:00:07,910
欢迎回来。 

4
00:00:07,910 --> 00:00:10,220
在这一集中，我们将做一些特别的事情， 

5
00:00:10,220 --> 00:00:13,164
那是从头开始编写我们自己的分类器。 

6
00:00:13,164 --> 00:00:14,580
如果你是机器学习的新手， 

7
00:00:14,580 --> 00:00:16,170
这是一个重要的里程碑。 

8
00:00:16,170 --> 00:00:18,560
因为如果你可以自己跟随并做到这一点， 

9
00:00:18,560 --> 00:00:21,890
这意味着你理解了这个难题的一个重要部分。 

10
00:00:21,890 --> 00:00:23,670
我们今天要写的分类器

11
00:00:23,670 --> 00:00:26,390
是k-Nearest Neighbors的杂乱版本。 

12
00:00:26,390 --> 00:00:29,660
这是最简单的分类器之一。 

13
00:00:29,660 --> 00:00:32,860
首先，这里简要介绍一下我们将在本集中做些什么。 

14
00:00:32,860 --> 00:00:35,170
我们将从第4集“Let”中的代码开始

15
00:00:35,170 --> 00:00:36,570
写一个管道。 

16
00:00:36,570 --> 00:00:39,290
回想一下那集我们做了一个简单的实验。 

17
00:00:39,290 --> 00:00:42,720
我们导入了一个数据集并将其拆分为训练和测试。 

18
00:00:42,720 --> 00:00:44,580
我们用火车来训练分类器， 

19
00:00:44,580 --> 00:00:47,150
并测试它是多么准确。 

20
00:00:47,150 --> 00:00:48,760
编写分类器就是其中的一部分

21
00:00:48,760 --> 00:00:50,940
我们今天要专注于今天。 

22
00:00:50,940 --> 00:00:52,739
以前我们导入了分类器

23
00:00:52,740 --> 00:00:55,280
来自使用这两行的图书馆。 

24
00:00:55,280 --> 00:00:58,190
在这里，我们将评论它们并编写我们自己的。 

25
00:00:58,190 --> 00:01:01,539
管道的其余部分将保持完全相同。 

26
00:01:01,539 --> 00:01:03,830
我会弹出进出截屏视频来解释一些事情

27
00:01:03,830 --> 00:01:05,940
我们一起去。 

28
00:01:05,940 --> 00:01:08,530
首先，让我们运行我们的管道来提醒自己

29
00:01:08,530 --> 00:01:10,120
准确度是多少。 

30
00:01:10,120 --> 00:01:12,370
如你所见，它超过90％。 

31
00:01:12,370 --> 00:01:14,260
这就是分类器的目标

32
00:01:14,260 --> 00:01:15,660
我们会自己写。 

33
00:01:15,660 --> 00:01:17,679
现在让我们注释掉那个导入。 

34
00:01:17,680 --> 00:01:19,540
蝙蝠，这打破了我们的代码。 

35
00:01:19,540 --> 00:01:22,250
所以我们需要做的第一件事是修复我们的管道。 

36
00:01:22,250 --> 00:01:24,850
为此，我们将为分类器实现一个类。 

37
00:01:24,850 --> 00:01:27,690


38
00:01:27,690 --> 00:01:29,580
我称之为ScrappyKNN。 

39
00:01:29,580 --> 00:01:31,690
通过斗志，我的意思是裸骨。 

40
00:01:31,690 --> 00:01:33,710
足以让它运作起来。 

41
00:01:33,710 --> 00:01:38,110
接下来，我将更改我们的管道以使用它。 

42
00:01:38,110 --> 00:01:40,880
现在让我们看看我们需要实现哪些方法。 

43
00:01:40,880 --> 00:01:42,949
查看分类器的界面， 

44
00:01:42,950 --> 00:01:45,500
我们看到有两个我们在乎 - 合身， 

45
00:01:45,500 --> 00:01:47,470
进行训练并预测， 

46
00:01:47,470 --> 00:01:49,270
做预测。 

47
00:01:49,270 --> 00:01:51,600
首先，我们将声明我们的拟合方法。 

48
00:01:51,600 --> 00:01:54,440
请记住，这会获取训练集的功能和标签

49
00:01:54,440 --> 00:01:57,679
作为输入，我们将为这些添加参数。 

50
00:01:57,680 --> 00:02:00,090
现在让我们继续我们的预测方法。 

51
00:02:00,090 --> 00:02:03,470
作为输入，它接收我们的测试数据的功能。 

52
00:02:03,470 --> 00:02:06,920
作为输出，它返回标签的预测。 

53
00:02:06,920 --> 00:02:09,389
我们的第一个目标是让管道工作， 

54
00:02:09,389 --> 00:02:12,230
并了解这些方法的作用。 

55
00:02:12,230 --> 00:02:14,179
所以在我们编写真正的分类器之前， 

56
00:02:14,180 --> 00:02:15,810
我们将从更简单的事情开始。 

57
00:02:15,810 --> 00:02:18,240
我们将编写一个随机分类器。 

58
00:02:18,240 --> 00:02:21,690
随机，我的意思是我们只是猜测标签。 

59
00:02:21,690 --> 00:02:25,310
首先，我们将为拟合和预测方法添加一些代码。 

60
00:02:25,310 --> 00:02:28,460
在拟合中，我会将训练数据存储在此课程中。 

61
00:02:28,460 --> 00:02:30,480
你可以把它想象成只是记住它。 

62
00:02:30,480 --> 00:02:32,840
你会明白为什么我们以后会这样做。 

63
00:02:32,840 --> 00:02:34,670
在预测方法中，请记住

64
00:02:34,670 --> 00:02:37,410
我们需要返回一个预测列表。 

65
00:02:37,410 --> 00:02:40,090
那是因为参数X_test实际上是

66
00:02:40,090 --> 00:02:42,730
 2D数组或列表列表。 

67
00:02:42,730 --> 00:02:46,410
每行包含一个测试示例的功能。 

68
00:02:46,410 --> 00:02:48,170
要对每一行进行预测， 

69
00:02:48,170 --> 00:02:50,829
我只是从训练数据中随机选择一个标签

70
00:02:50,830 --> 00:02:53,340
并将其附加到我们的预测中。 

71
00:02:53,340 --> 00:02:55,660
在这一点上，我们的管道再次运作。 

72
00:02:55,660 --> 00:02:58,029
所以，让我们运行它，看看它有多好。 

73
00:02:58,029 --> 00:03:00,070
回想一下有三种不同类型的花

74
00:03:00,070 --> 00:03:05,070
在虹膜数据集中，精度应该在33％左右。 

75
00:03:05,070 --> 00:03:07,109
现在我们知道了分类器的接口。 

76
00:03:07,110 --> 00:03:09,060
但是当我们开始这个练习时， 

77
00:03:09,060 --> 00:03:11,770
我们的准确率超过90％。 

78
00:03:11,770 --> 00:03:14,500
那么让我们看看我们是否可以做得更好。 

79
00:03:14,500 --> 00:03:16,850
为此，我们将实现我们的分类器， 

80
00:03:16,850 --> 00:03:19,380
这基于k-Nearest Neighbors。 

81
00:03:19,380 --> 00:03:22,469
这是该算法如何工作的直觉。 

82
00:03:22,469 --> 00:03:24,760
让我们回到我们的绿点和红点图纸

83
00:03:24,760 --> 00:03:26,399
从上一集开始。 

84
00:03:26,400 --> 00:03:27,990
想象一下我们在屏幕上看到的点

85
00:03:27,990 --> 00:03:30,880
是我们在拟合方法中记忆的训练数据， 

86
00:03:30,880 --> 00:03:33,420
对于玩具数据集说。 

87
00:03:33,420 --> 00:03:36,220
现在假设我们被要求对此测试做出预测

88
00:03:36,220 --> 00:03:38,260
我将在这里用灰色画出来。 

89
00:03:38,260 --> 00:03:39,959
我们怎么能这样做？ 

90
00:03:39,960 --> 00:03:41,890
那么在最近邻分类器中， 

91
00:03:41,890 --> 00:03:44,220
它听起来就像听起来一样。 

92
00:03:44,220 --> 00:03:45,940
我们会找到训练点

93
00:03:45,940 --> 00:03:48,070
最接近测试点。 

94
00:03:48,070 --> 00:03:50,392
这一点是最近的邻居。 

95
00:03:50,392 --> 00:03:51,850
然后我们会预测测试

96
00:03:51,850 --> 00:03:54,170
 point有相同的标签。 

97
00:03:54,170 --> 00:03:56,880
例如，我们猜测这个测试点是绿色的， 

98
00:03:56,880 --> 00:03:59,880
因为那是它最近邻居的颜色。 

99
00:03:59,880 --> 00:04:02,430
再举一个例子，如果我们在这里有一个测试点， 

100
00:04:02,430 --> 00:04:04,170
我们猜它是红色的。 

101
00:04:04,170 --> 00:04:06,399
那么这个正好在中间呢？ 

102
00:04:06,400 --> 00:04:08,730
想象一下，这个点与最近的点等距离

103
00:04:08,730 --> 00:04:10,750
绿点和最近的红点。 

104
00:04:10,750 --> 00:04:13,570
有一个平局，所以我们怎么能分类呢？ 

105
00:04:13,570 --> 00:04:15,959
那么一种方法是我们可以随机打破平局。 

106
00:04:15,960 --> 00:04:18,970
但还有另一种方式，那就是k进来的地方。 

107
00:04:18,970 --> 00:04:20,680
 K是我们考虑的邻居数量

108
00:04:20,680 --> 00:04:22,340
在做出预测时。 

109
00:04:22,340 --> 00:04:25,530
如果k为1，我们只看最近的训练点。 

110
00:04:25,530 --> 00:04:28,169
但如果k为3，我们会看三个最接近的。 

111
00:04:28,170 --> 00:04:30,860
在这种情况下，其中两个是绿色，一个是红色。 

112
00:04:30,860 --> 00:04:34,410
为了预测，我们可以投票和预测大多数班级。 

113
00:04:34,410 --> 00:04:36,230
现在有更详细的算法， 

114
00:04:36,230 --> 00:04:38,540
但这足以让我们开始。 

115
00:04:38,540 --> 00:04:40,200
要对此进行编码，首先我们需要一种方法

116
00:04:40,200 --> 00:04:42,700
找到最近的邻居。 

117
00:04:42,700 --> 00:04:44,840
要做到这一点，我们将衡量直线

118
00:04:44,840 --> 00:04:48,950
两点之间的距离，就像你用尺子做的那样。 

119
00:04:48,950 --> 00:04:52,099
有一个称为欧氏距离的公式， 

120
00:04:52,100 --> 00:04:54,310
这是公式的样子。 

121
00:04:54,310 --> 00:04:56,590
它测量两点之间的距离， 

122
00:04:56,590 --> 00:04:59,250
它有点像毕达哥拉斯定理。 

123
00:04:59,250 --> 00:05:02,350
平方加B平方等于C平方。 

124
00:05:02,350 --> 00:05:04,790
您可以将此术语视为A或差异

125
00:05:04,790 --> 00:05:06,840
介于前两个功能之间。 

126
00:05:06,840 --> 00:05:08,669
同样，您可以将此术语视为B， 

127
00:05:08,670 --> 00:05:11,170
或者第二对特征之间的区别。 

128
00:05:11,170 --> 00:05:14,740
我们计算的距离是斜边的长度。 

129
00:05:14,740 --> 00:05:16,460
现在这里很酷。 

130
00:05:16,460 --> 00:05:17,940
现在我们正在计算距离

131
00:05:17,940 --> 00:05:20,670
在二维空间，因为我们只有两个

132
00:05:20,670 --> 00:05:22,780
我们的玩具数据集中的功能。 

133
00:05:22,780 --> 00:05:25,969
但是，如果我们有三个特征或三个维度呢？ 

134
00:05:25,970 --> 00:05:28,200
那么我们就在一个立方体中。 

135
00:05:28,200 --> 00:05:30,450
我们仍然可以想象如何测量距离

136
00:05:30,450 --> 00:05:32,500
在带有尺子的空间里。 

137
00:05:32,500 --> 00:05:35,270
但是，如果我们有四个特征或四个维度， 

138
00:05:35,270 --> 00:05:36,710
就像我们在虹膜中做的那样？ 

139
00:05:36,710 --> 00:05:38,500
好吧，现在我们在一个超立方体中，我们

140
00:05:38,500 --> 00:05:40,740
无法想象这很容易。 

141
00:05:40,740 --> 00:05:42,450
好消息是欧几里德距离

142
00:05:42,450 --> 00:05:46,010
无论维数多少，都以相同的方式工作。 

143
00:05:46,010 --> 00:05:50,050
通过更多功能，我们可以在等式中添加更多术语。 

144
00:05:50,050 --> 00:05:52,060
您可以在线找到更多相关信息。 

145
00:05:52,060 --> 00:05:54,670


146
00:05:54,670 --> 00:05:56,770
现在让我们编码欧几里德距离。 

147
00:05:56,770 --> 00:05:58,270
有很多方法可以做到这一点， 

148
00:05:58,270 --> 00:06:00,370
但我们将使用一个名为scipy的库

149
00:06:00,370 --> 00:06:02,460
已经由Anaconda安装。 

150
00:06:02,460 --> 00:06:05,380
这里，A和B是数字特征的列表。 

151
00:06:05,380 --> 00:06:07,330
说A是我们的训练数据中的一点， 

152
00:06:07,330 --> 00:06:09,800
和B是我们测试数据的一点。 

153
00:06:09,800 --> 00:06:12,860
此函数返回它们之间的距离。 

154
00:06:12,860 --> 00:06:14,440
这就是我们需要的所有数学，所以现在

155
00:06:14,440 --> 00:06:17,390
让我们来看看分类器的算法。 

156
00:06:17,390 --> 00:06:19,479
要对测试点进行预测， 

157
00:06:19,480 --> 00:06:22,250
我们将计算到所有训练点的距离。 

158
00:06:22,250 --> 00:06:25,030
然后我们预测测试点具有相同的标签

159
00:06:25,030 --> 00:06:27,140
作为最接近的一个。 

160
00:06:27,140 --> 00:06:28,909
我会删除我们做的随机预测， 

161
00:06:28,910 --> 00:06:31,610
并用找到最接近的训练的方法替换它

162
00:06:31,610 --> 00:06:33,470
指向测试点。 

163
00:06:33,470 --> 00:06:35,580
对于这个视频很难，我会​​硬编码k到1， 

164
00:06:35,580 --> 00:06:38,090
所以我们正在写一个最近邻分类器。 

165
00:06:38,090 --> 00:06:40,099
 k变量不会出现在我们的代码中， 

166
00:06:40,100 --> 00:06:42,950
因为我们总能找到最接近的点。 

167
00:06:42,950 --> 00:06:45,700
在这种方法中，我们将遍历所有训练点

168
00:06:45,700 --> 00:06:48,250
并记录到目前为止最接近的一个。 

169
00:06:48,250 --> 00:06:50,650
请记住，我们记住了训练数据

170
00:06:50,650 --> 00:06:54,190
函数，X_train包含这些功能。 

171
00:06:54,190 --> 00:06:56,910
首先，我将计算距测试点的距离

172
00:06:56,910 --> 00:06:58,740
到第一个训练点。 

173
00:06:58,740 --> 00:07:00,990
我将使用此变量来跟踪最短的变量

174
00:07:00,990 --> 00:07:02,584
到目前为止我们找到的距离。 

175
00:07:02,584 --> 00:07:04,000
我将使用此变量来保持

176
00:07:04,000 --> 00:07:07,400
跟踪最接近的训练点的索引。 

177
00:07:07,400 --> 00:07:09,909
我们稍后需要它来检索它的标签。 

178
00:07:09,910 --> 00:07:12,370
现在我们将迭代所有其他训练点。 

179
00:07:12,370 --> 00:07:14,410
每当我们找到一个更接近的， 

180
00:07:14,410 --> 00:07:16,150
我们将更新我们的变量。 

181
00:07:16,150 --> 00:07:18,099
最后，我们将使用索引返回

182
00:07:18,100 --> 00:07:22,110
最接近的训练样例的标签。 

183
00:07:22,110 --> 00:07:24,780
此时，我们有一个工作的最近邻分类器， 

184
00:07:24,780 --> 00:07:29,510
所以让我们运行它，看看准确性是多少。 

185
00:07:29,510 --> 00:07:31,330
如你所见，它超过90％。 

186
00:07:31,330 --> 00:07:32,250
我们做到了。 

187
00:07:32,250 --> 00:07:34,240
当你自己运行时，准确性

188
00:07:34,240 --> 00:07:36,906
由于列车测试中的随机性，可能会有点不同

189
00:07:36,906 --> 00:07:38,530
分裂。 

190
00:07:38,530 --> 00:07:40,739
现在，如果您可以编写并理解它， 

191
00:07:40,740 --> 00:07:42,670
这是一个很大的成就，因为它

192
00:07:42,670 --> 00:07:46,254
意味着你可以从头开始编写一个简单的分类器。 

193
00:07:46,254 --> 00:07:47,920
现在，有一些优点和缺点

194
00:07:47,920 --> 00:07:50,850
对于这个算法，你可以在网上找到很多。 

195
00:07:50,850 --> 00:07:53,580
理解的基本原则是相对容易理解， 

196
00:07:53,580 --> 00:07:56,000
并且在某些问题上工作得相当好。 

197
00:07:56,000 --> 00:07:57,670
而基本的缺点是它很慢， 

198
00:07:57,670 --> 00:07:59,990
因为它必须迭代每个训练点

199
00:07:59,990 --> 00:08:01,550
做出预测。 

200
00:08:01,550 --> 00:08:04,070
而且重要的是，正如我们在第3集中所看到的， 

201
00:08:04,070 --> 00:08:06,384
某些功能比其他功能更丰富。 

202
00:08:06,384 --> 00:08:08,050
但是没有一种简单的表达方式

203
00:08:08,050 --> 00:08:10,120
在k-最近邻居。 

204
00:08:10,120 --> 00:08:12,150
从长远来看，我们需要一个分类器

205
00:08:12,150 --> 00:08:15,260
学习功能之间更复杂的关系

206
00:08:15,260 --> 00:08:17,289
以及我们试图预测的标签。 

207
00:08:17,290 --> 00:08:19,490
决策树就是一个很好的例子。 

208
00:08:19,490 --> 00:08:22,120
和我们在TensorFlow Playground中看到的神经网络一样

209
00:08:22,120 --> 00:08:23,730
甚至更好。 

210
00:08:23,730 --> 00:08:24,940
好的，希望这很有帮助。 

211
00:08:24,940 --> 00:08:26,227
一如既往地感谢您的收看。 

212
00:08:26,227 --> 00:08:28,560
您可以在Twitter上关注我的更新，当然， 

213
00:08:28,560 --> 00:08:29,310
 Google Developers。 

214
00:08:29,310 --> 00:08:32,190
我下次会见到你们。 

215
00:08:32,190 --> 00:08:35,540
 [音乐播放] 

216
00:08:35,539 --> 00:08:43,678


