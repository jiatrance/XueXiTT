1
00:00:00,000 --> 00:00:05,486


2
00:00:05,486 --> 00:00:06,609
 JOSH GORDON：嘿，大家好。 

3
00:00:06,610 --> 00:00:07,709
欢迎回来。 

4
00:00:07,709 --> 00:00:10,000
在这一集中，我们将编写一个决策树分类器

5
00:00:10,000 --> 00:00:12,560
从纯粹的Python开始。 

6
00:00:12,560 --> 00:00:14,420
这是我们将要介绍的内容的概述。 

7
00:00:14,420 --> 00:00:17,330
我将首先介绍我们将使用的数据集。 

8
00:00:17,330 --> 00:00:19,310
接下来，我们将预览已完成的树。 

9
00:00:19,310 --> 00:00:20,450
然后，我们将构建它。 

10
00:00:20,450 --> 00:00:23,030
在路上，我们将涵盖决策树学习等概念， 

11
00:00:23,030 --> 00:00:24,880
基尼杂质和信息增益。 

12
00:00:24,880 --> 00:00:26,630
你可以找到这一集的代码

13
00:00:26,630 --> 00:00:27,560
在描述中。 

14
00:00:27,560 --> 00:00:29,599
它有两种格式， 

15
00:00:29,600 --> 00:00:32,990
作为Jupiter笔记本和常规Python文件。 

16
00:00:32,990 --> 00:00:34,679
好的，让我们开始吧。 

17
00:00:34,679 --> 00:00:36,387
在这一集中，我写了一个玩具数据

18
00:00:36,387 --> 00:00:39,590
设置包括数字和分类属性。 

19
00:00:39,590 --> 00:00:42,050
在这里，我们的目标是预测水果的类型， 

20
00:00:42,050 --> 00:00:44,360
基于特征，像苹果或葡萄

21
00:00:44,360 --> 00:00:46,287
喜欢颜色和大小。 

22
00:00:46,287 --> 00:00:47,870
在剧集结束时，我鼓励

23
00:00:47,870 --> 00:00:50,239
你可以换掉你自己的数据集

24
00:00:50,240 --> 00:00:53,250
并为您关心的问题构建一棵树。 

25
00:00:53,250 --> 00:00:54,390
我们来看看格式。 

26
00:00:54,390 --> 00:00:56,420
为了清楚起见，我在这里重新绘制了它。 

27
00:00:56,420 --> 00:00:57,620
每一行都是一个例子。 

28
00:00:57,620 --> 00:01:00,230
前两列提供了功能或属性

29
00:01:00,230 --> 00:01:02,064
描述数据。 

30
00:01:02,064 --> 00:01:03,980
最后一列给出了标签或类， 

31
00:01:03,980 --> 00:01:05,229
我们想要预测。 

32
00:01:05,230 --> 00:01:07,310
如果您愿意，可以修改此数据集

33
00:01:07,310 --> 00:01:09,740
通过添加其他功能或更多示例， 

34
00:01:09,740 --> 00:01:12,095
我们的程序将以完全相同的方式工作。 

35
00:01:12,095 --> 00:01:13,970
现在，这个数据集非常简单， 

36
00:01:13,970 --> 00:01:15,470
除了一件事。 

37
00:01:15,470 --> 00:01:18,192
我写了它，所以它不是完全可分的。 

38
00:01:18,192 --> 00:01:20,149
而且我的意思是没有办法分辨

39
00:01:20,150 --> 00:01:22,340
第二和第五个例子。 

40
00:01:22,340 --> 00:01:24,650
它们具有相同的功能，但标签不同。 

41
00:01:24,650 --> 00:01:27,555
这是我们可以看到我们的树如何处理这种情况。 

42
00:01:27,555 --> 00:01:29,180
到了笔记本的末尾，你会

43
00:01:29,180 --> 00:01:31,040
以相同的格式查找测试数据。 

44
00:01:31,040 --> 00:01:33,200


45
00:01:33,200 --> 00:01:35,700
现在我编写了一些实用程序函数，使其更容易

46
00:01:35,700 --> 00:01:36,947
使用此数据。 

47
00:01:36,947 --> 00:01:39,030
在每个函数下面，我写了一个小演示

48
00:01:39,030 --> 00:01:40,351
展示它是如何工作的。 

49
00:01:40,351 --> 00:01:42,600
我已经为每个代码块重复了这种模式

50
00:01:42,600 --> 00:01:45,476
在笔记本中。 

51
00:01:45,477 --> 00:01:47,810
现在要构建树，我们使用决策树学习

52
00:01:47,810 --> 00:01:49,299
算法称为CART。 

53
00:01:49,299 --> 00:01:51,590
事实上，有一整套算法

54
00:01:51,590 --> 00:01:53,404
用于从数据构建树。 

55
00:01:53,404 --> 00:01:55,070
他们的核心是给你一个程序

56
00:01:55,070 --> 00:01:58,429
决定询问哪些问题以及何时提问。 

57
00:01:58,430 --> 00:02:01,190
 CART代表分类和回归树。 

58
00:02:01,190 --> 00:02:04,039
这是预览它的工作原理。 

59
00:02:04,040 --> 00:02:06,500
首先，我们将为树添加根节点。 

60
00:02:06,500 --> 00:02:09,080
并且所有节点都接收行列表作为输入。 

61
00:02:09,080 --> 00:02:12,090
根将收到整个训练集。 

62
00:02:12,090 --> 00:02:14,150
现在每个节点都会提出一个真正的错误问题

63
00:02:14,150 --> 00:02:16,010
关于其中一个功能。 

64
00:02:16,010 --> 00:02:17,630
并回答这个问题， 

65
00:02:17,630 --> 00:02:21,299
我们将数据拆分或分区为两个子集。 

66
00:02:21,300 --> 00:02:24,080
然后，这些子集成为两个子节点的输入

67
00:02:24,080 --> 00:02:25,970
我们加入树。 

68
00:02:25,970 --> 00:02:28,520
问题的目标是取消混合标签

69
00:02:28,520 --> 00:02:30,050
当我们继续前进

70
00:02:30,050 --> 00:02:32,600
或者换句话说，产生最纯粹的可能

71
00:02:32,600 --> 00:02:35,552
在每个节点分配标签。 

72
00:02:35,552 --> 00:02:37,010
例如，此节点的输入

73
00:02:37,010 --> 00:02:39,410
仅包含一种类型的标签， 

74
00:02:39,410 --> 00:02:41,570
所以我们说它完全没有混合。 

75
00:02:41,570 --> 00:02:44,630
标签的类型没有不确定性。 

76
00:02:44,630 --> 00:02:47,540
另一方面，此节点中的标签仍然混淆， 

77
00:02:47,540 --> 00:02:50,510
所以我们会问另一个问题，以进一步缩小范围。 

78
00:02:50,510 --> 00:02:52,399
以及构建有效树的技巧

79
00:02:52,400 --> 00:02:55,590
是要了解要问的问题和时间。 

80
00:02:55,590 --> 00:02:58,340
要做到这一点，我们需要量化一个问题

81
00:02:58,340 --> 00:03:00,216
有助于取消混合标签。 

82
00:03:00,216 --> 00:03:02,090
我们可以量化不确定性的数量

83
00:03:02,090 --> 00:03:05,176
在单个节点上使用称为Gini杂质的度量。 

84
00:03:05,176 --> 00:03:06,799
我们可以量化一个问题

85
00:03:06,800 --> 00:03:09,140
使用概念减少不确定性

86
00:03:09,140 --> 00:03:11,100
称为信息增益。 

87
00:03:11,100 --> 00:03:12,769
我们将使用这些来选择最好的

88
00:03:12,770 --> 00:03:14,540
要问每个问题。 

89
00:03:14,540 --> 00:03:17,150
鉴于这个问题，我们将递归地构建树

90
00:03:17,150 --> 00:03:18,840
在每个新节点上。 

91
00:03:18,840 --> 00:03:20,840
我们将继续将数据分割到那里

92
00:03:20,840 --> 00:03:23,330
在这一点上，没有其他问题要问

93
00:03:23,330 --> 00:03:24,680
我们将添加一片叶子。 

94
00:03:24,680 --> 00:03:26,630
要实现这一点，首先我们需要了解

95
00:03:26,630 --> 00:03:29,120
我们可以询问有关数据的哪类问题。 

96
00:03:29,120 --> 00:03:30,680
其次，我们需要了解

97
00:03:30,680 --> 00:03:32,770
如何确定何时提出问题。 

98
00:03:32,770 --> 00:03:35,520


99
00:03:35,520 --> 00:03:37,980
现在每个节点都将一个行列表作为输入。 

100
00:03:37,980 --> 00:03:39,660
并生成一个问题列表

101
00:03:39,660 --> 00:03:42,359
我们将迭代每个特征的每个值

102
00:03:42,360 --> 00:03:44,166
出现在那些行中。 

103
00:03:44,166 --> 00:03:45,540
这些中的每一个都成为候选人

104
00:03:45,540 --> 00:03:48,239
对于我们可以用来分区数据的阈值。 

105
00:03:48,240 --> 00:03:51,120
并且通常会有很多可能性。 

106
00:03:51,120 --> 00:03:52,950
在代码中，我们通过存储来表示问题

107
00:03:52,950 --> 00:03:55,170
列号和列值， 

108
00:03:55,170 --> 00:03:58,035
或者我们将用于分区数据的阈值。 

109
00:03:58,035 --> 00:03:59,910
例如，这是我们如何写一个问题

110
00:03:59,910 --> 00:04:01,850
测试颜色是否为绿色。 

111
00:04:01,850 --> 00:04:03,930
这是一个数字属性的例子

112
00:04:03,930 --> 00:04:07,620
测试直径是否大于或等于3。 

113
00:04:07,620 --> 00:04:10,320
在回答问题时，我们对数据进行划分或分区

114
00:04:10,320 --> 00:04:12,060
分成两个子集。 

115
00:04:12,060 --> 00:04:14,880
第一个包含问题为真的所有行。 

116
00:04:14,880 --> 00:04:17,610
第二个包含其他一切。 

117
00:04:17,610 --> 00:04:19,740
在代码中，我们的分区函数提出了一个问题

118
00:04:19,740 --> 00:04:21,720
和行列表作为输入。 

119
00:04:21,720 --> 00:04:24,060
例如，这是我们如何基于行进行分区

120
00:04:24,060 --> 00:04:26,950
关于颜色是否为红色。 

121
00:04:26,950 --> 00:04:29,214
这里，true行包含所有红色示例。 

122
00:04:29,214 --> 00:04:30,880
假行包含其他所有内容。 

123
00:04:30,880 --> 00:04:33,885


124
00:04:33,886 --> 00:04:36,260
最好的问题是减少我们不确定性的问题

125
00:04:36,260 --> 00:04:37,219
最多。 

126
00:04:37,220 --> 00:04:39,800
而基尼的杂质让我们量化了多少不确定性

127
00:04:39,800 --> 00:04:41,180
有一个节点。 

128
00:04:41,180 --> 00:04:43,190
信息增益将让我们量化多少

129
00:04:43,190 --> 00:04:44,960
一个问题减少了。 

130
00:04:44,960 --> 00:04:46,880
让我们先研究杂质。 

131
00:04:46,880 --> 00:04:49,490
现在，这是一个介于0和1之间的指标

132
00:04:49,490 --> 00:04:52,190
其中较低的值表示较少的不确定性或混合， 

133
00:04:52,190 --> 00:04:53,540
在一个节点。 

134
00:04:53,540 --> 00:04:56,480
如果我们随机，它会量化我们错误的可能性

135
00:04:56,480 --> 00:05:00,560
将集合中的标签分配给该集合中的示例。 

136
00:05:00,560 --> 00:05:02,940
这是一个清楚的例子。 

137
00:05:02,940 --> 00:05:06,240
想象一下，我们有两个碗，一个包含例子

138
00:05:06,240 --> 00:05:08,430
另一个包含标签。 

139
00:05:08,430 --> 00:05:11,760
首先，我们将从第一个碗中随机抽取一个例子。 

140
00:05:11,760 --> 00:05:14,130
然后我们将从第二个中随机绘制一个标签。 

141
00:05:14,130 --> 00:05:17,580
现在，我们将示例分类为具有该标签。 

142
00:05:17,580 --> 00:05:21,330
基尼杂质给了我们错误的机会。 

143
00:05:21,330 --> 00:05:23,969
在这个例子中，我们每个碗中只有苹果。 

144
00:05:23,970 --> 00:05:25,480
没有办法犯错误。 

145
00:05:25,480 --> 00:05:28,335
所以我们说杂质是零。 

146
00:05:28,335 --> 00:05:30,710
另一方面，给出了五种不同类型的碗

147
00:05:30,710 --> 00:05:32,590
水果比例相等，我们

148
00:05:32,590 --> 00:05:35,119
说它的杂质为0.8。 

149
00:05:35,120 --> 00:05:37,790
那是因为我们有五分之一的机会是正确的

150
00:05:37,790 --> 00:05:41,650
如果我们将一个标签随机分配给一个例子。 

151
00:05:41,650 --> 00:05:44,474
在代码中，此方法计算数据集的杂质。 

152
00:05:44,474 --> 00:05:45,890
我写了几个例子

153
00:05:45,890 --> 00:05:48,110
下面说明它是如何工作的。 

154
00:05:48,110 --> 00:05:50,360
您可以看到第一组的杂质为零

155
00:05:50,360 --> 00:05:51,770
因为没有混合。 

156
00:05:51,770 --> 00:05:57,010
在这里，你可以看到杂质是0.8。 

157
00:05:57,010 --> 00:05:59,890
现在，信息获取将让我们找到减少的问题

158
00:05:59,890 --> 00:06:01,318
我们的不确定性最大。 

159
00:06:01,319 --> 00:06:02,860
它只是一个描述的数字

160
00:06:02,860 --> 00:06:06,340
一个问题有多少有助于解除节点上的标签。 

161
00:06:06,340 --> 00:06:07,909
这是个主意。 

162
00:06:07,910 --> 00:06:09,580
我们首先计算不确定性

163
00:06:09,580 --> 00:06:11,229
我们的起始集。 

164
00:06:11,230 --> 00:06:12,750
那么，对于我们可以问的每个问题， 

165
00:06:12,750 --> 00:06:15,160
我们将尝试分区数据和计算

166
00:06:15,160 --> 00:06:18,087
导致子节点的不确定性。 

167
00:06:18,087 --> 00:06:20,170
我们将对其不确定性进行加权平均

168
00:06:20,170 --> 00:06:22,870
因为我们更关心一个低不确定性的大型集合

169
00:06:22,870 --> 00:06:25,667
而不是一个高的小集。 

170
00:06:25,667 --> 00:06:28,000
然后，我们将从我们的起始不确定性中减去这一点。 

171
00:06:28,000 --> 00:06:29,722
这就是我们的信息收益。 

172
00:06:29,722 --> 00:06:31,680
随着我们的进展，我们将跟踪问题

173
00:06:31,680 --> 00:06:33,300
产生最大的收益。 

174
00:06:33,300 --> 00:06:36,570
这将是这个节点最好的问题。 

175
00:06:36,570 --> 00:06:38,460
让我们看一下代码中的外观。 

176
00:06:38,460 --> 00:06:41,409
在这里，我们将迭代功能的每个值。 

177
00:06:41,409 --> 00:06:43,200
我们将为该功能生成一个问题， 

178
00:06:43,200 --> 00:06:45,750
然后对其上的数据进行分区。 

179
00:06:45,750 --> 00:06:49,259
请注意，我们会丢弃任何未能产生拆分的问题。 

180
00:06:49,259 --> 00:06:51,050
然后，我们将计算我们的信息收益。 

181
00:06:51,050 --> 00:06:52,424
在这个功能里面，你可以

182
00:06:52,424 --> 00:06:56,107
我们看一下加权平均值和每组的杂质。 

183
00:06:56,107 --> 00:06:57,940
我们看到这减少了多少不确定性

184
00:06:57,940 --> 00:06:59,350
从我们的起始集。 

185
00:06:59,350 --> 00:07:01,850
我们会追踪最佳价值。 

186
00:07:01,850 --> 00:07:04,510
我也在下面写了几个演示。 

187
00:07:04,510 --> 00:07:07,560
好的，有了这些概念，我们就可以构建树了。 

188
00:07:07,560 --> 00:07:10,060
把这一切放在一起我认为最有用的东西我

189
00:07:10,060 --> 00:07:11,890
能做的就是引导你完成算法

190
00:07:11,890 --> 00:07:14,200
因为它为我们的训练数据构建了一棵树。 

191
00:07:14,200 --> 00:07:17,770
这使用递归，因此在行动中查看它可能会有所帮助。 

192
00:07:17,770 --> 00:07:21,490
您可以在Build Tree功能中找到此代码。 

193
00:07:21,490 --> 00:07:23,350
当我们第一次调用构建树时， 

194
00:07:23,350 --> 00:07:26,050
它接收整个训练集作为输入。 

195
00:07:26,050 --> 00:07:27,940
作为输出，它将返回一个参考

196
00:07:27,940 --> 00:07:30,340
到我们树的根节点。 

197
00:07:30,340 --> 00:07:33,039
我将在这里用灰色绘制一个占位符。 

198
00:07:33,040 --> 00:07:35,280
这是我们在这个节点上考虑的行。 

199
00:07:35,280 --> 00:07:38,380
首先，这是整个训练集。 

200
00:07:38,380 --> 00:07:40,960
现在我们在这个节点上找到最好的问题。 

201
00:07:40,960 --> 00:07:44,810
我们通过迭代这些值中的每一个来做到这一点。 

202
00:07:44,810 --> 00:07:47,180
我们将分割数据并计算信息

203
00:07:47,180 --> 00:07:48,560
每个人都获得了。 

204
00:07:48,560 --> 00:07:50,690
随着我们的进展，我们将跟踪问题

205
00:07:50,690 --> 00:07:53,790
产生最大的收益。 

206
00:07:53,790 --> 00:07:55,980
现在在这种情况下，有一个有用的问题要问， 

207
00:07:55,980 --> 00:07:57,960
所以增益将大于零。 

208
00:07:57,960 --> 00:08:00,750
我们将使用该问题拆分数据。 

209
00:08:00,750 --> 00:08:03,780
现在，我们将通过再次调用构建树来使用递归

210
00:08:03,780 --> 00:08:06,479
为真正的分支添加节点。 

211
00:08:06,479 --> 00:08:08,520
我们现在考虑的行是上半部分

212
00:08:08,520 --> 00:08:09,570
分裂。 

213
00:08:09,570 --> 00:08:13,170
再一次，我们会找到最好的问题来询问这些数据。 

214
00:08:13,170 --> 00:08:15,870
我们再一次拆分并调用构建树函数

215
00:08:15,870 --> 00:08:17,910
添加子节点。 

216
00:08:17,910 --> 00:08:20,400
现在，对于这些数据，没有其他问题要问。 

217
00:08:20,400 --> 00:08:22,289
因此信息增益将为零。 

218
00:08:22,290 --> 00:08:24,562
而这个节点变成了叶子。 

219
00:08:24,562 --> 00:08:26,270
它将预测一个例子是

220
00:08:26,270 --> 00:08:28,820
 50％信心的苹果或柠檬

221
00:08:28,820 --> 00:08:32,460
因为那是数据中标签的比例。 

222
00:08:32,460 --> 00:08:34,640
现在我们将继续构建假分支。 

223
00:08:34,640 --> 00:08:36,860
在这里，这也将成为一片叶子。 

224
00:08:36,860 --> 00:08:40,590
我们将100％放心地预测苹果。 

225
00:08:40,590 --> 00:08:42,480
现在前一个调用返回，并且此节点

226
00:08:42,480 --> 00:08:44,986
成为决策节点。 

227
00:08:44,986 --> 00:08:46,860
在代码中，这意味着它拥有引用

228
00:08:46,860 --> 00:08:49,975
我们问的问题和结果的两个子节点。 

229
00:08:49,975 --> 00:08:52,020
我们差不多完成了。 

230
00:08:52,020 --> 00:08:55,230
现在我们返回根节点并构建false分支。 

231
00:08:55,230 --> 00:08:58,200
没有其他问题要问，所以这就成了一片叶子。 

232
00:08:58,200 --> 00:09:00,860
这预示着葡萄100％的自信。 

233
00:09:00,860 --> 00:09:04,050
最后，根节点也成为决策节点。 

234
00:09:04,050 --> 00:09:06,786
我们对构建树的调用返回对它的引用。 

235
00:09:06,787 --> 00:09:08,370
如果你在代码中向下滚动，你会

236
00:09:08,370 --> 00:09:10,740
看到我添加了分类数据和打印的功能

237
00:09:10,740 --> 00:09:11,490
那个树。 

238
00:09:11,490 --> 00:09:13,594
这些开始于对根节点的引用， 

239
00:09:13,594 --> 00:09:14,760
所以你可以看到它是如何工作的。 

240
00:09:14,760 --> 00:09:17,610


241
00:09:17,610 --> 00:09:18,850
好的，希望这很有帮助。 

242
00:09:18,850 --> 00:09:21,340
您可以查看代码以获取更多详细信息。 

243
00:09:21,340 --> 00:09:23,680
关于决策树我还有很多话要说， 

244
00:09:23,680 --> 00:09:25,954
但是我们可以在短时间内完成这么多工作。 

245
00:09:25,955 --> 00:09:28,330
以下是一些值得注意的主题。 

246
00:09:28,330 --> 00:09:30,413
您可以查看说明中的书籍

247
00:09:30,413 --> 00:09:31,420
了解更多。 

248
00:09:31,420 --> 00:09:33,430
下一步，我建议修改树

249
00:09:33,430 --> 00:09:35,223
使用您自己的数据集。 

250
00:09:35,224 --> 00:09:36,640
这可能是一种有趣的构建方式

251
00:09:36,640 --> 00:09:38,830
一个简单易懂的分类器供使用

252
00:09:38,830 --> 00:09:40,011
在你的项目中。 

253
00:09:40,011 --> 00:09:41,260
谢谢各位的观看。 

254
00:09:41,260 --> 00:09:43,290
我下次再见。 

255
00:09:43,290 --> 00:09:52,038


