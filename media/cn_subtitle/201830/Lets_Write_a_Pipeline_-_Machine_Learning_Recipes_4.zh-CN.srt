1
00:00:00,000 --> 00:00:02,844
[音乐]

2
00:00:06,640 --> 00:00:07,447
欢迎回来

3
00:00:07,447 --> 00:00:09,030
我们已经讨论了一些基础知识，

4
00:00:09,030 --> 00:00:11,879
所以今天我要回顾并强化概念。

5
00:00:11,890 --> 00:00:13,950
为此，我们要进行两项探索

6
00:00:13,950 --> 00:00:16,090
首先，我们会编码一个基本管道

7
00:00:16,090 --> 00:00:17,640
进行监督学习。

8
00:00:17,640 --> 00:00:19,390
我会向大家展示多个分类器

9
00:00:19,390 --> 00:00:21,280
如何解决同一个问题。

10
00:00:21,280 --> 00:00:23,200
然后，我们要锐化直觉

11
00:00:23,200 --> 00:00:25,710
关于一个算法从数据中学习的真正含义，

12
00:00:25,710 --> 00:00:29,502
因为尽管听起来这很魔幻，实际上一点也不

13
00:00:29,502 --> 00:00:31,710
为了扫平障碍，我们来看一个常见的

14
00:00:31,710 --> 00:00:33,010
你可能想要进行的实验

15
00:00:33,010 --> 00:00:35,210
想象你在建立一个垃圾邮件分类器。

16
00:00:35,210 --> 00:00:37,510
这仅仅是一个功能，
把刚收到的邮件标记为

17
00:00:37,510 --> 00:00:39,306
垃圾邮件或正常邮件。

18
00:00:39,307 --> 00:00:41,140
现在，你已经收集了一些数据，

19
00:00:41,140 --> 00:00:42,850
这已经为训练一个模型做好准备。

20
00:00:42,850 --> 00:00:44,460
但在使用它之前，

21
00:00:44,460 --> 00:00:46,760
首先需要回答一个问题：

22
00:00:46,760 --> 00:00:49,820
这个模型究竟可以做到多准确

23
00:00:49,820 --> 00:00:51,740
如果用它来分类你的数据中没有的邮件。

24
00:00:51,740 --> 00:00:54,850
我们希望尽可能地确认模型工作正常，

25
00:00:54,850 --> 00:00:56,490
在使用它之前。

26
00:00:56,490 --> 00:00:59,290
我们可以通过一个实验来帮助检验这一点。

27
00:00:59,500 --> 00:01:02,930
一个办法是把已有的数据分成两部分。

28
00:01:02,930 --> 00:01:05,080
我们称之为训练数据和测试数据。

29
00:01:05,080 --> 00:01:07,010
我们用训练数据来训练我们的模型，

30
00:01:07,010 --> 00:01:10,380
使用测试数据
来测试模型在新数据上运行的准确度。

31
00:01:10,380 --> 00:01:13,890
这是一个常见的模式，
让我们来看看怎么用代码实现。

32
00:01:13,890 --> 00:01:17,060
为了扫除障碍，
我们往scikit中导入数据集。

33
00:01:17,060 --> 00:01:20,020
我们要再次用Iris，
它的数据已经包含其中 非常方便。

34
00:01:20,020 --> 00:01:21,960
我们已经在第二集中见到过Iris。

35
00:01:21,960 --> 00:01:23,559
但我们之前没有见到的是

36
00:01:23,560 --> 00:01:26,832
我将两个术语称作 
特征x与标签y。

37
00:01:26,832 --> 00:01:28,210
为什么呢？

38
00:01:28,210 --> 00:01:30,669
这是因为看待分类器的一种方式

39
00:01:30,670 --> 00:01:32,230
是看成一种函数。

40
00:01:32,230 --> 00:01:34,750
在更高层面，你可以认为x是输入，

41
00:01:34,750 --> 00:01:36,500
y是输出。

42
00:01:36,500 --> 00:01:39,892
我会在后半集进一步讲述这部分。

43
00:01:39,892 --> 00:01:42,350
我们导入了数据集，第一步要做的

44
00:01:42,350 --> 00:01:44,589
就是把它分成训练数据和测试数据。

45
00:01:44,590 --> 00:01:46,640
为此，我们可以导入一个方便的实体，

46
00:01:46,640 --> 00:01:48,530
让句法更清晰。

47
00:01:48,530 --> 00:01:50,340
我们现在定义了我们的x和y，

48
00:01:50,340 --> 00:01:52,930
特征和标签，然后把它们分开

49
00:01:52,930 --> 00:01:54,450
成两个集合。

50
00:01:54,450 --> 00:01:55,980
x_train和 y_train

51
00:01:55,980 --> 00:01:57,980
是训练集的特征和标签。

52
00:01:57,980 --> 00:01:59,500
x_test 和y_test

53
00:01:59,500 --> 00:02:02,032
是测试集的特征和标签。

54
00:02:02,032 --> 00:02:04,240
这里，我要将一半的数据

55
00:02:04,240 --> 00:02:05,580
用作测试。

56
00:02:05,580 --> 00:02:07,639
所以，在Iris中我们有150个样本，

57
00:02:07,639 --> 00:02:11,299
75个用来训练，75个用来测试。

58
00:02:11,530 --> 00:02:13,280
现在我们要建立分类器。

59
00:02:13,280 --> 00:02:14,980
这里我要用两种类型

60
00:02:14,980 --> 00:02:17,859
来向大家展示如何实现同一个任务。

61
00:02:17,860 --> 00:02:20,500
首先我们来用已经见过的决策树。

62
00:02:20,500 --> 00:02:22,240
请注意：这里有两行代码，

63
00:02:22,240 --> 00:02:23,448
它们仅适用于分类器。

64
00:02:25,650 --> 00:02:28,830
现在让我们来用我们的训练集来训练分类器。

65
00:02:28,830 --> 00:02:31,600
到这里，它已经可以用来分类数据了。

66
00:02:31,600 --> 00:02:33,329
然后，我们会调用预测方法

67
00:02:33,330 --> 00:02:35,805
并用它来分类测试数据。

68
00:02:35,805 --> 00:02:37,180
如果你打印出来预测结果，

69
00:02:37,180 --> 00:02:38,970
就会发现有一串数字。

70
00:02:38,970 --> 00:02:40,660
它们分别对应于Iris的类型，

71
00:02:40,660 --> 00:02:44,010
是分类器对测试数据每一行进行的预测。

72
00:02:44,010 --> 00:02:46,230
现在让我们来看看我们的分类器

73
00:02:46,230 --> 00:02:48,280
在测试集上的运行是否准确。

74
00:02:48,280 --> 00:02:50,840
向前回顾，我们有测试数据的真实标签。

75
00:02:51,650 --> 00:02:53,460
为了计算准确性，

76
00:02:53,460 --> 00:02:55,760
我们可以将预测的标签与真实的标签比较

77
00:02:55,760 --> 00:02:57,349
计算出得分。

78
00:02:57,349 --> 00:02:59,140
对此，scikit中有个比较方便的方法

79
00:02:59,140 --> 00:03:00,829
我们可以导入。

80
00:03:00,830 --> 00:03:03,505
看这里，我们的准确性超过90%。

81
00:03:03,505 --> 00:03:06,130
如果你自己尝试一遍，结果可能略有不同

82
00:03:06,130 --> 00:03:08,269
因为在训练/测试数据的划分上

83
00:03:08,270 --> 00:03:10,040
存在一定的随机性。

84
00:03:10,040 --> 00:03:11,880
这是比较有趣的一点。

85
00:03:11,880 --> 00:03:14,690
通过取代这两行，我们可以用另一种分类器

86
00:03:14,690 --> 00:03:16,920
来完成相同的任务。

87
00:03:16,920 --> 00:03:18,570
我们不使用决策树，

88
00:03:18,570 --> 00:03:20,930
而是用一个叫KNearestNeighbors的程序

89
00:03:20,930 --> 00:03:22,500
如果我们运行程序，

90
00:03:22,500 --> 00:03:25,354
我们发现代码运行起来和前面几乎一样。

91
00:03:25,354 --> 00:03:27,270
当你自己运行时结果可能有所不同，

92
00:03:27,270 --> 00:03:29,800
因为这个分类器工作方式略有不同

93
00:03:29,800 --> 00:03:32,440
还有划分训练/测试数据的随机性。

94
00:03:32,440 --> 00:03:35,420
类似地，如果我们要用一种更加复杂的分类器，

95
00:03:35,420 --> 00:03:38,220
我们只需要导入它，然后改变这两行。

96
00:03:38,220 --> 00:03:40,297
与此同时，其他部分保持不变。

97
00:03:40,297 --> 00:03:42,880
另外要注意的是，
尽管有各种类型的分类器

98
00:03:42,880 --> 00:03:45,920
在更高层次上，
它们拥有相似的接口。

99
00:03:49,059 --> 00:03:50,850
现在我们深入讲述

100
00:03:50,850 --> 00:03:53,120
从数据中学习意味着什么。

101
00:03:53,120 --> 00:03:56,080
之前，我说过特征x与标签y，

102
00:03:56,080 --> 00:03:58,717
它们是一个函数的输入和输出。

103
00:03:58,717 --> 00:04:00,800
当然，函数是一种

104
00:04:00,800 --> 00:04:02,190
我们在编程中已经了解的东西。

105
00:04:02,190 --> 00:04:04,900
def classify--这就是函数。

106
00:04:04,900 --> 00:04:06,920
正如我们在监督学习中了解的，

107
00:04:06,920 --> 00:04:09,059
我们并不想亲自去写上这些东西。

108
00:04:09,060 --> 00:04:12,360
我们想要一个算法来从训练数据中学习。

109
00:04:12,360 --> 00:04:15,240
所以学习一个函数是什么意思呢？

110
00:04:15,240 --> 00:04:17,120
函数仅仅是一种映射，

111
00:04:17,120 --> 00:04:18,660
从输入到输出值。

112
00:04:18,660 --> 00:04:20,660
这是一个函数，你之前可能看到过——

113
00:04:20,660 --> 00:04:22,700
y=mx+b。

114
00:04:22,700 --> 00:04:24,820
这是一条直线的方程，

115
00:04:24,820 --> 00:04:27,340
有两个参数：m代表斜率，

116
00:04:27,340 --> 00:04:29,679
b代表y轴截距。

117
00:04:29,680 --> 00:04:31,110
给定这些参数，

118
00:04:31,110 --> 00:04:34,320
我们就可以画出不同x值得函数值。

119
00:04:34,320 --> 00:04:36,610
现在，在监督学习下，我们的分类函数

120
00:04:36,610 --> 00:04:38,420
也可能有一些参数，

121
00:04:38,420 --> 00:04:42,300
但输入x是一个要分类样本的特征

122
00:04:42,300 --> 00:04:44,390
输出y则是一个标签，

123
00:04:44,390 --> 00:04:47,219
像垃圾邮件、正常邮件，或者花的种类。

124
00:04:47,220 --> 00:04:49,661
那么函数主体是什么样的呢？

125
00:04:49,661 --> 00:04:51,910
这部分就是我们要写的算法，

126
00:04:51,910 --> 00:04:53,737
或者说，学习算法。

127
00:04:53,737 --> 00:04:55,320
在这里，我们需要理解的重要一点

128
00:04:55,320 --> 00:04:57,130
即我们并非从头开始

129
00:04:57,130 --> 00:05:00,060
从无到有得到函数主体。

130
00:05:00,060 --> 00:05:01,990
相反，我们从模型开始。

131
00:05:01,990 --> 00:05:04,050
你可以把模型当成一个原型

132
00:05:04,050 --> 00:05:07,030
用做定义我们函数主体的规则。

133
00:05:07,030 --> 00:05:08,539
一般的，一个模型具有参数，

134
00:05:08,540 --> 00:05:10,290
参数根据训练数据作调整。

135
00:05:10,290 --> 00:05:14,560
关于这部分如何运行，来看这个高级的例子。

136
00:05:14,560 --> 00:05:17,380
我们来看一个玩具数据集，想想什么样的模型

137
00:05:17,380 --> 00:05:19,210
可以用作分类器。

138
00:05:19,210 --> 00:05:20,960
假设我们想要区分

139
00:05:20,960 --> 00:05:22,690
红点和绿点，

140
00:05:22,690 --> 00:05:25,080
有一些我已经画出来了。

141
00:05:25,080 --> 00:05:27,210
为此，我们需要用到两种特征

142
00:05:27,210 --> 00:05:29,450
每个点的x轴和y轴坐标。

143
00:05:29,450 --> 00:05:32,670
现在考虑该如何区分这两种点。

144
00:05:32,670 --> 00:05:34,090
我们需要一个函数，

145
00:05:34,090 --> 00:05:35,799
当有一个新的没见过的点到来，

146
00:05:35,800 --> 00:05:38,170
可以区分它是红的还是绿的。

147
00:05:38,170 --> 00:05:40,990
事实上，我们可能有很多的数据想要分类。

148
00:05:40,990 --> 00:05:42,840
这里我画了一些测试样本

149
00:05:42,840 --> 00:05:44,960
用浅红色和浅绿色。

150
00:05:44,960 --> 00:05:47,210
这些点并不存在于训练数据中。

151
00:05:47,210 --> 00:05:49,789
分类器也从没见过他们，

152
00:05:49,790 --> 00:05:51,700
那它是怎么预测出正确的标签呢？

153
00:05:51,700 --> 00:05:53,820
想象我们可以以一种方式画一条线

154
00:05:53,820 --> 00:05:56,037
像这样穿过数据。

155
00:05:56,037 --> 00:05:57,620
然后我们可以说

156
00:05:57,620 --> 00:06:00,090
线左侧的是绿的，右侧的是红的。

157
00:06:00,920 --> 00:06:03,430
这条线可以当成一个分类器。

158
00:06:03,430 --> 00:06:05,610
所以我们怎么学习这条线呢？

159
00:06:05,610 --> 00:06:08,240
一个办法就是利用训练数据

160
00:06:08,240 --> 00:06:09,880
来调整模型的参数。

161
00:06:09,880 --> 00:06:12,830
而且我们认为使用的模型是一条简单的直线

162
00:06:12,830 --> 00:06:14,460
如之前所示。

163
00:06:14,460 --> 00:06:17,830
也就是说我们有两个参数要调整：m和b。

164
00:06:17,830 --> 00:06:21,050
通过改变它们，我们可以改变直线所在的位置。

165
00:06:21,050 --> 00:06:23,500
那么我们如何学习得到正确的参数呢？

166
00:06:23,500 --> 00:06:25,690
一个想法是通过迭代

167
00:06:25,690 --> 00:06:27,640
利用训练数据来调整得到。

168
00:06:27,640 --> 00:06:29,890
比如，初始时我们用一条随机的直线，

169
00:06:29,890 --> 00:06:32,810
然后用它来分类第一个训练数据。

170
00:06:32,810 --> 00:06:35,370
如果是正确的，就不用改变直线，

171
00:06:35,370 --> 00:06:36,969
接着分类下一个训练数据。

172
00:06:36,969 --> 00:06:38,760
而相反，如果出错，

173
00:06:38,760 --> 00:06:41,300
我们可以轻微地改变模型的参数

174
00:06:41,300 --> 00:06:43,070
使之更准确。

175
00:06:43,070 --> 00:06:44,680
这一点需格外注意。

176
00:06:44,680 --> 00:06:46,100
看待学习的一种方式

177
00:06:46,130 --> 00:06:49,710
就是用训练数据调整模型的参数。

178
00:06:50,980 --> 00:06:52,950
这里是非常特殊的一点，

179
00:06:52,950 --> 00:06:55,270
称为TensorFlow游戏场。

180
00:06:55,270 --> 00:06:57,370
这是神经网络的一个优美范例，

181
00:06:57,370 --> 00:07:00,020
你可以直接在浏览器上运行与实验。

182
00:07:00,020 --> 00:07:02,060
这部分值得安排单独的一集，

183
00:07:02,060 --> 00:07:03,730
但现在就可以去体验一下它。

184
00:07:03,730 --> 00:07:04,930
非常赞。

185
00:07:04,930 --> 00:07:06,630
游乐场自带了不同的数据集

186
00:07:06,630 --> 00:07:08,300
供你来尝试。

187
00:07:08,300 --> 00:07:09,470
有一些非常简单。

188
00:07:09,470 --> 00:07:12,620
例如，我们可以用直线来区分这个。

189
00:07:12,620 --> 00:07:15,980
有一些数据相当复杂。

190
00:07:15,980 --> 00:07:17,620
这个数据集非常难。

191
00:07:17,620 --> 00:07:20,357
来看看你是否可以建立一个网络来区分它。

192
00:07:20,357 --> 00:07:21,940
现在你可以考虑一个神经网络

193
00:07:21,940 --> 00:07:24,170
当作一个复杂的分类器，

194
00:07:24,170 --> 00:07:26,430
像决策树或简单直线。

195
00:07:26,430 --> 00:07:29,191
但理论上，思想是相似的。

196
00:07:29,690 --> 00:07:30,687
好的。希望对大家有用。

197
00:07:30,687 --> 00:07:32,520
我刚创建了一个推特账号大家可以关注

198
00:07:32,520 --> 00:07:33,834
来接收新一集的更新提示。

199
00:07:33,834 --> 00:07:36,000
下一集将在几周后更新，

200
00:07:36,000 --> 00:07:37,990
取决于我在谷歌I/O上的工作量。

201
00:07:37,990 --> 00:07:40,890
谢谢大家的观看，下集再见。

