1
00:00:06,765 --> 00:00:08,140
戈登(Josh Gordon)：分类器的优秀之处

2
00:00:08,140 --> 00:00:10,270
只在于你所提供的各样不同特性

3
00:00:10,270 --> 00:00:12,059
这意味着具备好的特性

4
00:00:12,060 --> 00:00:14,740
是你在机器学习里
其中最重要的工作

5
00:00:14,740 --> 00:00:17,060
但是什么是好的特性
且你怎么能知道?

6
00:00:17,060 --> 00:00:19,400
如果你正在做二分类

7
00:00:19,400 --> 00:00:21,669
那么一个好的特性

8
00:00:21,670 --> 00:00:23,270
使它可以很容易在两个
不同的事物之间作决定

9
00:00:23,270 --> 00:00:26,100
例如假设我们要撰写一个分类器

10
00:00:26,100 --> 00:00:29,090
用来分辨两种类型的狗

11
00:00:29,090 --> 00:00:30,890
灰猎犬及拉布拉多犬

12
00:00:30,890 --> 00:00:34,090
在这里我们将使用两个特征
狗狗的身高以英寸为单位

13
00:00:34,090 --> 00:00:35,490
及它们眼睛的颜色

14
00:00:35,490 --> 00:00:38,490
用这个玩具作为例子
让我们做一些有关狗的假设

15
00:00:38,490 --> 00:00:40,930
使事情变得简单

16
00:00:40,930 --> 00:00:43,050
首先我们会说灰猎犬通常

17
00:00:43,050 --> 00:00:44,180
比拉布拉多犬高

18
00:00:44,180 --> 00:00:47,019
其次我们会假设狗狗的眼睛

19
00:00:47,020 --> 00:00:48,750
只有两种颜色—蓝色和棕色

20
00:00:48,750 --> 00:00:50,760
我们会说它们眼睛的颜色

21
00:00:50,760 --> 00:00:53,160
不是取决于品种

22
00:00:53,160 --> 00:00:55,519
这意味着其中的一个特点是有用的

23
00:00:55,520 --> 00:00:57,480
而另一个没有告诉我们任何事

24
00:00:57,480 --> 00:01:01,260
要理解其中原因
我们将建立一个玩具数据集

25
00:01:01,260 --> 00:01:02,970
使其可视化

26
00:01:02,970 --> 00:01:04,300
我们从身高开始

27
00:01:04,300 --> 00:01:06,649
你们认为这个特点有何作用?

28
00:01:06,650 --> 00:01:08,070
平均而言灰猎犬

29
00:01:08,070 --> 00:01:11,309
往往比拉布拉多犬高几英寸
但并非总是如此

30
00:01:11,310 --> 00:01:13,736
在世界上有很多不同的变异

31
00:01:13,736 --> 00:01:15,110
当我们想到一个特性时

32
00:01:15,110 --> 00:01:17,620
必须考虑它怎样在一个种群中

33
00:01:17,620 --> 00:01:19,630
寻找不同的值

34
00:01:19,630 --> 00:01:22,360
以Python程式为例

35
00:01:22,360 --> 00:01:24,440
我创建了一个1000只狗的种群

36
00:01:24,440 --> 00:01:27,737
50-50 灰猎犬-拉布拉多犬

37
00:01:27,737 --> 00:01:29,070
每一只狗我也会给它一个身高

38
00:01:29,070 --> 00:01:31,500
在这个例子中我们会说

39
00:01:31,500 --> 00:01:35,510
灰猎犬的平均身高为28英寸
而拉布拉多犬身高为24英寸

40
00:01:35,510 --> 00:01:37,564
现在所有狗狗都有点不同

41
00:01:37,564 --> 00:01:39,479
身高是正态分布

42
00:01:39,480 --> 00:01:42,790
然后我们会给它们加或减4英寸

43
00:01:42,790 --> 00:01:44,660
我们将会得到两个数字阵列

44
00:01:44,660 --> 00:01:47,200
然后利用矩形图使其可视化

45
00:01:47,200 --> 00:01:49,520
我将会添加一个参数
灰猎犬是红色

46
00:01:49,520 --> 00:01:51,320
拉布拉多犬为蓝色

47
00:01:51,320 --> 00:01:53,320
现在我们可以运行脚本

48
00:01:53,320 --> 00:01:57,460
这样显示出一个给定的身高有多少只狗

49
00:01:57,460 --> 00:01:58,960
在屏幕上有很多的数据

50
00:01:58,960 --> 00:02:03,202
让我们将其简化
逐个阅读

51
00:02:03,202 --> 00:02:05,230
我们从最左边的狗狗开始

52
00:02:05,230 --> 00:02:08,600
它们身高大约都是20英寸

53
00:02:08,600 --> 00:02:11,380
想象一下 如果我请大家预测
这样身高的狗狗是一只

54
00:02:11,380 --> 00:02:13,299
拉布拉多犬还是灰猎犬

55
00:02:13,300 --> 00:02:14,180
你会怎么做?

56
00:02:14,180 --> 00:02:16,710
你们可以断定在特定的身高

57
00:02:16,710 --> 00:02:18,670
每种类型狗狗的概率

58
00:02:18,670 --> 00:02:20,940
在这里它较可能是
一只拉布拉多犬

59
00:02:20,940 --> 00:02:22,966
另一方面

60
00:02:22,967 --> 00:02:24,550
如果我们看看柱状图的右面

61
00:02:24,550 --> 00:02:26,950
看看身高35英寸的狗狗

62
00:02:26,950 --> 00:02:29,450
我们相当有信心这是一只灰猎犬

63
00:02:29,450 --> 00:02:31,299
现在中间的狗狗又怎么样?

64
00:02:31,300 --> 00:02:33,520
你可以看到图表给我们较少信息

65
00:02:33,520 --> 00:02:36,750
每种类型的狗狗的概率很接近

66
00:02:36,750 --> 00:02:40,220
因此身高是一个有用的特征
但它并不完美

67
00:02:40,220 --> 00:02:42,280
所以在机器学习里

68
00:02:42,280 --> 00:02:43,482
你们总是需要多种的特征

69
00:02:43,482 --> 00:02:45,440
否则你们只是撰写一个if语句

70
00:02:45,440 --> 00:02:47,160
而不是分类器

71
00:02:47,160 --> 00:02:50,590
要断定使用哪个特点

72
00:02:50,590 --> 00:02:52,390
就需要做一个假想实验

73
00:02:52,390 --> 00:02:53,820
假设你是一个分类器

74
00:02:53,820 --> 00:02:55,870
你要弄清楚这是一只

75
00:02:55,870 --> 00:03:00,167
拉布拉多犬还是灰猎犬
你还想知道什么信息呢?

76
00:03:00,167 --> 00:03:01,750
你可能会问它们的头发长度

77
00:03:01,750 --> 00:03:04,680
它们跑得多快
或者它们的体重是多少

78
00:03:04,680 --> 00:03:06,980
到底应该使用多少特性

79
00:03:06,980 --> 00:03:08,549
是一门艺术而不是一门科学

80
00:03:08,550 --> 00:03:10,720
但作为一个经验法则
想想你需要多少特征

81
00:03:10,720 --> 00:03:12,620
来解决问题

82
00:03:12,620 --> 00:03:14,690
现在我们看另一特征
例如眼睛的颜色

83
00:03:14,690 --> 00:03:17,470
在这个例子

84
00:03:17,470 --> 00:03:20,500
我们假设狗狗的眼睛
只有两种颜色—蓝色和棕色

85
00:03:20,500 --> 00:03:22,100
我们假定眼睛的颜色

86
00:03:22,100 --> 00:03:24,500
并不取决于狗的品种

87
00:03:24,500 --> 00:03:28,590
像这样的例子可以用下面矩形图表示

88
00:03:28,590 --> 00:03:32,170
对于大部分数值，分布大约是50/50

89
00:03:32,170 --> 00:03:33,850
所以这个特征
没有告诉我们任何有用信息

90
00:03:33,850 --> 00:03:36,109
因为它跟狗狗的类型没有关联

91
00:03:36,110 --> 00:03:39,200
假设训练数据内包含
一个像这样无用的特征

92
00:03:39,200 --> 00:03:41,940
能破坏分类器的准确度

93
00:03:41,940 --> 00:03:43,200
因为很有可能

94
00:03:43,200 --> 00:03:46,209
虽然机器学习输出了准确结果
但那纯属偶然

95
00:03:46,210 --> 00:03:50,040
特别在训练数据不多的情况下

96
00:03:50,040 --> 00:03:52,320
你还希望各样特征是独立的

97
00:03:52,320 --> 00:03:54,600
独立特征给你

98
00:03:54,600 --> 00:03:56,870
不同类型的信息

99
00:03:56,870 --> 00:03:59,360
试想一下我们已经有一个特征
在我们的数据集里—

100
00:03:59,360 --> 00:04:00,800
身高及英寸

101
00:04:00,800 --> 00:04:02,250
问问你自己 如果增加
另一个特征会有所帮助吗

102
00:04:02,250 --> 00:04:05,800
例如用多少厘米表示身高?

103
00:04:05,800 --> 00:04:08,230
没有用
因为它跟已有的特征

104
00:04:08,230 --> 00:04:09,410
密切相关

105
00:04:09,410 --> 00:04:12,650
从训练数据删除高度相关的特征

106
00:04:12,650 --> 00:04:14,032
这是很好的做法

107
00:04:14,032 --> 00:04:15,490
因为很多分类器

108
00:04:15,490 --> 00:04:18,190
没有足够的智能明白

109
00:04:18,190 --> 00:04:20,200
用厘米或英寸量度身高
是同样的事情

110
00:04:20,200 --> 00:04:23,340
因此它们可能会重复
计算特征的重要性

111
00:04:23,340 --> 00:04:26,599
最后你们或者希望特征很容易被理解

112
00:04:26,600 --> 00:04:28,730
用一个新的例子

113
00:04:28,730 --> 00:04:30,330
假如你把一封信从一个城市寄到另一个城市

114
00:04:30,330 --> 00:04:33,580
你预测需要多少天

115
00:04:33,580 --> 00:04:37,130
两个城市之间的距离越远
所需时间也就越长

116
00:04:37,130 --> 00:04:39,650
一个有用的特征是

117
00:04:39,650 --> 00:04:42,200
用英里表示两个城市之间的距离

118
00:04:42,200 --> 00:04:44,219
一个较差的特征就是

119
00:04:44,220 --> 00:04:47,160
使用城市的经度及纬度

120
00:04:47,160 --> 00:04:48,260
去标示其位置

121
00:04:48,260 --> 00:04:48,969
这就是其原因

122
00:04:48,970 --> 00:04:51,120
我可以看看距离

123
00:04:51,120 --> 00:04:54,100
然后猜测要多久
信件才会到达目的地

124
00:04:54,100 --> 00:04:56,880
但是知道经纬度及时间之间的关联后

125
00:04:56,880 --> 00:05:00,020
也很难猜测有关时间

126
00:05:00,020 --> 00:05:01,986
并需要更多有关训练数据的例子

127
00:05:01,986 --> 00:05:03,360
现在你需要使用不同技巧来断定

128
00:05:03,360 --> 00:05:05,970
到底哪个特征为何对你有帮助

129
00:05:05,970 --> 00:05:08,920
甚至哪样的组合是最好的

130
00:05:08,920 --> 00:05:11,390
那么你不再需要听天由命

131
00:05:11,390 --> 00:05:13,770
我们在下一节再讨论

132
00:05:13,770 --> 00:05:16,229
接下来的时间我们将会继续

133
00:05:16,230 --> 00:05:17,750
为监督式学习建立直觉

134
00:05:17,750 --> 00:05:19,680
我们将会展示怎样使用
不同类型的分类器

135
00:05:19,680 --> 00:05:22,290
来解决同样的问题

136
00:05:22,290 --> 00:05:24,240
并深入了解它们怎样运作

137
00:05:24,240 --> 00:05:27,220
谢谢收看
我们下次再见

