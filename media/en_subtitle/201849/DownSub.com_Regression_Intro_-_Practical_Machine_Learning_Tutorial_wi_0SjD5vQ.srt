1
00:00:00,900 --> 00:00:07,780
Alright, so now we are at least going to get started with setting up a simple linear regression example.

2
00:00:08,160 --> 00:00:16,200
The first thing that we need to make sure we have is scikit learn, Pandas and Quandl. So open up terminal, command prompt, whatever.

3
00:00:16,500 --> 00:00:25,380
And pip install sklearn. pip install quandl. And pip install pandas.

4
00:00:25,380 --> 00:00:32,000
Once you have all those, you are good to go. So to install those, go ahead and pause the video and pick back up once you have them.

5
00:00:35,220 --> 00:00:41,460
Ok, so once you have those, let's go ahead and get started with a simple example. So we are starting with regression,

6
00:00:41,620 --> 00:00:44,720
and the idea of regression is to take continuous data,

7
00:00:44,740 --> 00:00:47,960
and figure out a best fit line to that data.

8
00:00:47,960 --> 00:00:49,760
and basically with that just boils down to

9
00:00:49,900 --> 00:00:53,220
we are trying to like "model" your data

10
00:00:53,220 --> 00:00:56,700
and the way we do that with regression at least with simple linear regression

11
00:00:56,700 --> 00:01:01,620
is just with a straight line so the equation of the line as we will talk more about down the line

12
00:01:01,700 --> 00:01:05,680
but as you might remember from school, y=mx+b,

13
00:01:05,680 --> 00:01:09,880
so if you have x, you can figure out what y is, also if you have m and b.

14
00:01:10,180 --> 00:01:15,020
So basically the whole point of regression is to find out what m an b is.

15
00:01:15,220 --> 00:01:19,680
So for example, a lot of people use regression with stock  prices so that's what we are gonna do

16
00:01:19,700 --> 00:01:21,060
at least in this one.

17
00:01:21,220 --> 00:01:29,000
And, so the idea is, this is continuous data and you've got months and months of stock prices and

18
00:01:29,000 --> 00:01:31,260
and each price is in its own kind of unique day.

19
00:01:31,260 --> 00:01:36,200
But all the data is kind of one dataset together as opposed to with like classification,

20
00:01:36,380 --> 00:01:40,260
where each group of data has its own unique label.

21
00:01:40,260 --> 00:01:44,240
So with machine learning, basically everything boils down, at least with supervised machine learning,

22
00:01:44,240 --> 00:01:47,980
everything boils down to features and labels.

23
00:01:47,980 --> 00:01:51,860
Features are like your attributes, or in this case, the continuous data.

24
00:01:52,220 --> 00:01:57,660
So, let's go ahead and get started and we'll talk a little bit more about features.

25
00:01:57,760 --> 00:02:03,620
So first of all, let's go ahead and import pandas as pd.

26
00:02:03,680 --> 00:02:06,120
And then we are gonna import Quandl with  a capital Q.

27
00:02:06,240 --> 00:02:09,520
And then what we are gonna say is df for dataframe

28
00:02:09,680 --> 00:02:12,540
equals Quandl.get and we'll put in the ticker.

29
00:02:12,560 --> 00:02:14,120
You can get this from quandl.

30
00:02:14,120 --> 00:02:15,880
So if you just go to quandl.com

31
00:02:15,960 --> 00:02:19,040
You can use a little search and find stuff like

32
00:02:19,040 --> 00:02:21,500
if I say google stock

33
00:02:21,500 --> 00:02:23,340
we can probably find it.

34
00:02:23,800 --> 00:02:28,020
Let's see I am trying to find, we are using the wiki dataset

35
00:02:28,660 --> 00:02:32,340
Let's just do free. Anyway, when you find it you can find

36
00:02:32,340 --> 00:02:34,780
all kinds of different datasets here but we are looking

37
00:02:34,780 --> 00:02:36,000
just simply for the wiki one.

38
00:02:39,200 --> 00:02:39,920
Here it is.

39
00:02:43,100 --> 00:02:44,820
You will pick up a dataset and you can

40
00:02:44,820 --> 00:02:45,720
come over here

41
00:02:45,720 --> 00:02:46,740
You can either just download here or more importantly

42
00:02:46,740 --> 00:02:49,980
here is the quandl code and then you can click on like

43
00:02:49,980 --> 00:02:52,680
python and this is the exact statement to get it.

44
00:02:53,300 --> 00:02:56,940
If you have an account, you can make basically unlimited

45
00:02:56,980 --> 00:02:59,640
request free data. If you don't use an account,

46
00:02:59,820 --> 00:03:01,540
like we are not gonna use an account here,

47
00:03:01,540 --> 00:03:03,420
like we are not gonna use an auth token.

48
00:03:03,520 --> 00:03:06,700
If you don't have an account, I think, it's limited like 50 calls a day.

49
00:03:06,860 --> 00:03:10,460
We are actually only use quandl fairly short term here

50
00:03:10,540 --> 00:03:13,080
and then maybe later on. So you really don't

51
00:03:13,100 --> 00:03:14,980
need to create an account, but if you like quandl,

52
00:03:15,200 --> 00:03:17,860
you might as well make an account at some point.

53
00:03:17,880 --> 00:03:23,340
So anyways, quandl.get and then wiki/google was the ticker there

54
00:03:23,840 --> 00:03:25,920
so then we can just simply print

55
00:03:25,920 --> 00:03:27,140
let's print the df.head

56
00:03:27,140 --> 00:03:29,780
just so we can see what it is we are working with.

57
00:03:31,020 --> 00:03:34,060
We'll see that basically each column here

58
00:03:34,100 --> 00:03:36,140
is a feature.

59
00:03:36,140 --> 00:03:39,980
So the open, high, low, close, these are features.

60
00:03:40,320 --> 00:03:46,780
So in machine learning you can have all the features you want but you want to have meaningful features

61
00:03:46,960 --> 00:03:51,480
features that actually have something to do with your data

62
00:03:51,540 --> 00:03:55,280
So some people are pretty avid believers in the ideas like

63
00:03:55,280 --> 00:03:57,060
pattern recognition with stock prices

64
00:03:57,680 --> 00:03:59,280
and that might be you but

65
00:03:59,360 --> 00:04:02,020
do you need every single one of these

66
00:04:02,020 --> 00:04:03,900
open high low close columns to do

67
00:04:03,900 --> 00:04:05,660
pattern recognition? No.

68
00:04:06,460 --> 00:04:08,520
Also, you would know we've got

69
00:04:08,520 --> 00:04:10,380
open high low close volume and then adjusted

70
00:04:10,380 --> 00:04:14,140
and adjusted is adjusted after a thing like stock splits

71
00:04:14,380 --> 00:04:15,420
so a stock split

72
00:04:15,420 --> 00:04:17,579
maybe your company has 10 stocks

73
00:04:17,579 --> 00:04:19,919
and each stock is $1000 a share

74
00:04:19,920 --> 00:04:25,120
and you decide I want people to be able to buy shares of my company for less than $1000

75
00:04:25,120 --> 00:04:27,220
So you might say, ok, BAM,

76
00:04:27,220 --> 00:04:29,140
every share is now two shares

77
00:04:29,140 --> 00:04:32,220
so we have 20 total shares and the share price is $500

78
00:04:32,220 --> 00:04:36,100
so you have adjusted prices to account for that

79
00:04:36,100 --> 00:04:39,320
so it doesn't like like the stock price went from $1000 to $500

80
00:04:39,700 --> 00:04:41,360
so that's what adjusted is

81
00:04:41,360 --> 00:04:42,260
so we are gonna be using those

82
00:04:42,360 --> 00:04:44,940
but again, each one of these is really related to the other one

83
00:04:45,440 --> 00:04:49,120
like the correlation of these two columns is super high

84
00:04:49,820 --> 00:04:52,020
so would you use each one of these columns

85
00:04:52,020 --> 00:04:56,340
Does that the next one really brings that much meaningful data? No

86
00:04:56,420 --> 00:04:58,620
but one thing to always think about

87
00:04:58,620 --> 00:05:00,880
when you have features and labels is maybe like

88
00:05:00,880 --> 00:05:03,340
what about the relationship between those columns

89
00:05:03,340 --> 00:05:07,460
so when we get into something like deep learning and then some of the other algorithms

90
00:05:07,580 --> 00:05:13,660
you can start to discover relationships between attributes

91
00:05:13,880 --> 00:05:17,240
but with regression, just simply no.

92
00:05:20,260 --> 00:05:21,560
what you wanna do

93
00:05:21,560 --> 00:05:24,100
you wanna like simplify your data as much as possible.

94
00:05:24,100 --> 00:05:26,980
You want as many meaningful features as you can get

95
00:05:27,020 --> 00:05:31,100
but useless features as we'll show kinda through this series

96
00:05:31,340 --> 00:05:35,400
can really cause a lot of trouble for your machine learning classifiers

97
00:05:35,400 --> 00:05:38,580
especially the more simple ones in supervised learning and so on

98
00:05:38,580 --> 00:05:42,580
anyways let's close out of this and let's go ahead and grab some features

99
00:05:43,180 --> 00:05:45,260
what we're gonna say

100
00:05:45,260 --> 00:05:47,080
first we are gonna pair this down,

101
00:05:47,080 --> 00:05:49,860
we are gonna say dataframe equals the df

102
00:05:50,100 --> 00:05:57,600
and then we are going to create a long list basically all of the columns that we wanna have

103
00:05:57,660 --> 00:06:03,400
so we are gonna take adjusted, open, and then I'm just gonna go ahead and copy this

104
00:06:03,940 --> 00:06:06,380
copy, ok

105
00:06:06,600 --> 00:06:10,040
so that's adjusted, open, and then we are gonna take

106
00:06:10,080 --> 00:06:16,980
oepn, so high, low, close, and volume

107
00:06:16,980 --> 00:06:20,360
ok so now we have just these columns so we

108
00:06:20,360 --> 00:06:26,640
kinda recreated our dataframe to just be the open high low close and volume of the adjusted ones.

109
00:06:26,640 --> 00:06:31,540
so then, like I was saying, some of these columns are relatively worthless

110
00:06:31,540 --> 00:06:34,420
but they do have some relationships

111
00:06:34,420 --> 00:06:37,460
so for example, like what is interesting about high and low

112
00:06:37,820 --> 00:06:42,540
is the margin of high and low tells us

113
00:06:42,540 --> 00:06:45,340
a little bit about volatility for the day

114
00:06:45,480 --> 00:06:49,400
Also, the open price that's the starting price for the day

115
00:06:49,400 --> 00:06:51,900
and it's relationship to the close price

116
00:06:51,900 --> 00:06:53,560
tells us did the price go up

117
00:06:53,560 --> 00:06:54,680
if so, by how much

118
00:06:54,680 --> 00:06:56,180
and did it go down? If so, by how much and so on.

119
00:06:56,280 --> 00:06:59,460
so the relationship there is very valuable.

120
00:06:59,460 --> 00:07:02,060
But a simple linear regression is not gonna seek out that

121
00:07:02,060 --> 00:07:04,780
relationship. It's just gonna work with whatever

122
00:07:04,780 --> 00:07:06,120
features you feed through it

123
00:07:06,140 --> 00:07:12,840
so what we need to do is define those special relationships

124
00:07:12,840 --> 00:07:14,820
and then use those as our features rather than

125
00:07:14,860 --> 00:07:17,920
redundant almost prices that not gonna really give us

126
00:07:17,920 --> 00:07:20,000
anything else very useful

127
00:07:20,520 --> 00:07:24,100
first let's do the high minus the low percent

128
00:07:24,100 --> 00:07:26,200
so this is like the percent volatility almost

129
00:07:26,340 --> 00:07:28,580
so we are gonna define a new column

130
00:07:28,580 --> 00:07:31,360
we are gonna call it HL_percent

131
00:07:31,460 --> 00:07:33,400
and then that is going to be

132
00:07:33,420 --> 00:07:35,560
I'm having a hard time here

133
00:07:35,560 --> 00:07:37,480
that's gonna be equal to

134
00:07:37,600 --> 00:07:38,880
so percent change is

135
00:07:40,160 --> 00:07:42,520
in this case it would be the high minus low

136
00:07:42,580 --> 00:07:44,900
divided by the low times 100

137
00:07:44,900 --> 00:07:56,700
so for us it would be df Adj high minus the df Adj close

138
00:07:56,700 --> 00:07:59,300
and what's happening here is just on a per row basis

139
00:07:59,300 --> 00:08:01,640
which is just this column minus this column

140
00:08:02,280 --> 00:08:12,080
so that column divided by df Adj close and then times 100

141
00:08:12,180 --> 00:08:14,340
you can either times by 100 or not

142
00:08:14,460 --> 00:08:17,560
the classifier really is not gonna care about that

143
00:08:17,620 --> 00:08:19,400
we are just doing that for ourselves

144
00:08:19,400 --> 00:08:20,920
So that's the high minus low percent

145
00:08:20,920 --> 00:08:23,600
and then we actually want just the daily percent change

146
00:08:23,620 --> 00:08:25,380
like the daily move

147
00:08:25,380 --> 00:08:27,920
so I'm just gonna copy that whole line, paste

148
00:08:27,920 --> 00:08:34,100
and then we are gonna call this one percent_change

149
00:08:34,100 --> 00:08:38,299
and that is equal to pretty much the same thing only we need to change some stuff

150
00:08:38,299 --> 00:08:43,659
so normally percent change is new minus the old divided by the old times 100

151
00:08:43,659 --> 00:08:48,060
so that would be adjusted close minus adjusted open

152
00:08:48,060 --> 00:08:49,480
so new minus the old

153
00:08:49,480 --> 00:08:52,560
divided by the old times 100

154
00:08:52,560 --> 00:08:55,280
oh, I am sorry, ha, we did it the wrong way.

155
00:08:55,320 --> 00:09:00,600
divided by the old, so this would be open times 100

156
00:09:00,600 --> 00:09:01,920
so that's percent change.

157
00:09:01,920 --> 00:09:04,240
actually, you can pass close here again

158
00:09:04,240 --> 00:09:09,020
the classifier doesn't really care as long as everything kinda normalized

159
00:09:09,060 --> 00:09:11,460
but yea so either way would been fine

160
00:09:11,460 --> 00:09:14,100
this is the actual way you should do it anyways

161
00:09:15,060 --> 00:09:17,620
once we have that data

162
00:09:17,700 --> 00:09:20,400
we are gonna define a new dataframe

163
00:09:20,400 --> 00:09:22,600
and we are instead gonna say

164
00:09:22,660 --> 00:09:26,680
so it's gonna be df equals df[]

165
00:09:26,680 --> 00:09:28,840
and then now we define the only columns that

166
00:09:28,840 --> 00:09:30,100
we really acutally care about

167
00:09:30,740 --> 00:09:34,340
and so in our case the columns we care about are gonna be

168
00:09:34,980 --> 00:09:43,700
adjusted close, the high low percent, the percent change

169
00:09:44,200 --> 00:09:48,260
and then volume is also somewhat useful to have

170
00:09:48,460 --> 00:09:53,200
so volume is just how many trades were occurred basically that day

171
00:09:54,160 --> 00:09:57,200
so volume is also kinda related to volatility

172
00:09:57,200 --> 00:10:01,340
so you can also make more features with some sort of relationship there

173
00:10:01,340 --> 00:10:02,760
but we'll try to keep this pretty simple

174
00:10:02,760 --> 00:10:05,080
so for now we'll just print df.head

175
00:10:06,720 --> 00:10:10,080
and we wait just to make sure everything worked out

176
00:10:10,080 --> 00:10:11,080
and sure enough it did

177
00:10:11,080 --> 00:10:12,860
so we have all the numbers we are kinda interested in

178
00:10:13,340 --> 00:10:16,420
so we got our features and eventually

179
00:10:16,420 --> 00:10:20,420
this will actually wound up being, possibly, our label

180
00:10:20,420 --> 00:10:23,040
but we'll get to..., I guess think about

181
00:10:23,040 --> 00:10:24,940
between now and the next tutorial

182
00:10:25,060 --> 00:10:30,380
features are the kinda of the attributes that make up the label

183
00:10:30,380 --> 00:10:33,060
and the label is, hopefully, some sort of prediction

184
00:10:33,060 --> 00:10:34,100
into the future

185
00:10:34,660 --> 00:10:38,100
so will the adjusted close, will this column,

186
00:10:38,100 --> 00:10:40,920
actually be a feature? or will it be a label

187
00:10:40,960 --> 00:10:42,480
as it stands right now.

188
00:10:42,480 --> 00:10:44,840
so think about that and the next tutorial

189
00:10:44,840 --> 00:10:47,440
we'll pick it up and start getting closer actually

190
00:10:47,440 --> 00:10:49,740
making real predictions with this data

191
00:10:49,740 --> 00:10:53,300
so if you have any questions, comments, whatever, leave them below

192
00:10:53,300 --> 00:00:00,000
otherwise, as always, thanks for watching, thanks for all the supports, subscriptions and until next time

