1
00:00:00,000 --> 00:00:02,844
[MUSIC PLAYING]

2
00:00:02,844 --> 00:00:06,640


3
00:00:06,640 --> 00:00:07,447
Welcome back.

4
00:00:07,447 --> 00:00:09,030
We've covered a lot
of ground already,

5
00:00:09,030 --> 00:00:12,070
so today I want to review
and reinforce concepts.

6
00:00:12,070 --> 00:00:14,250
To do that, we'll
explore two things.

7
00:00:14,250 --> 00:00:16,090
First, we'll code
up a basic pipeline

8
00:00:16,090 --> 00:00:17,640
for supervised learning.

9
00:00:17,640 --> 00:00:19,390
I'll show you how
multiple classifiers

10
00:00:19,390 --> 00:00:21,280
can solve the same problem.

11
00:00:21,280 --> 00:00:23,200
Next, we'll build up a
little more intuition

12
00:00:23,200 --> 00:00:25,710
for what it means for an
algorithm to learn something

13
00:00:25,710 --> 00:00:29,502
from data, because that sounds
kind of magical, but it's not.

14
00:00:29,502 --> 00:00:31,710
To kick things off, let's
look at a common experiment

15
00:00:31,710 --> 00:00:33,010
you might want to do.

16
00:00:33,010 --> 00:00:35,210
Imagine you're building
a spam classifier.

17
00:00:35,210 --> 00:00:37,510
That's just a function that
labels an incoming email

18
00:00:37,510 --> 00:00:39,306
as spam or not spam.

19
00:00:39,307 --> 00:00:41,140
Now, say you've already
collected a data set

20
00:00:41,140 --> 00:00:42,850
and you're ready
to train a model.

21
00:00:42,850 --> 00:00:44,460
But before you put
it into production,

22
00:00:44,460 --> 00:00:46,760
there's a question you
need to answer first--

23
00:00:46,760 --> 00:00:49,820
how accurate will it be when you
use it to classify emails that

24
00:00:49,820 --> 00:00:51,740
weren't in your training data?

25
00:00:51,740 --> 00:00:54,850
As best we can, we want to
verify our models work well

26
00:00:54,850 --> 00:00:56,490
before we deploy them.

27
00:00:56,490 --> 00:00:59,290
And we can do an experiment
to help us figure that out.

28
00:00:59,290 --> 00:01:02,930
One approach is to partition
our data set into two parts.

29
00:01:02,930 --> 00:01:05,080
We'll call these Train and Test.

30
00:01:05,080 --> 00:01:07,010
We'll use Train
to train our model

31
00:01:07,010 --> 00:01:10,380
and Test to see how
accurate it is on new data.

32
00:01:10,380 --> 00:01:13,890
That's a common pattern, so
let's see how it looks in code.

33
00:01:13,890 --> 00:01:17,060
To kick things off, let's import
a data set into [? SyKit. ?]

34
00:01:17,060 --> 00:01:20,020
We'll use Iris again, because
it's handily included.

35
00:01:20,020 --> 00:01:21,960
Now, we already saw
Iris in episode two.

36
00:01:21,960 --> 00:01:23,559
But what we haven't
seen before is

37
00:01:23,560 --> 00:01:26,832
that I'm calling the
features x and the labels y.

38
00:01:26,832 --> 00:01:28,210
Why is that?

39
00:01:28,210 --> 00:01:30,669
Well, that's because one
way to think of a classifier

40
00:01:30,670 --> 00:01:32,230
is as a function.

41
00:01:32,230 --> 00:01:34,750
At a high level, you can
think of x as the input

42
00:01:34,750 --> 00:01:36,500
and y as the output.

43
00:01:36,500 --> 00:01:39,892
I'll talk more about that in
the second half of this episode.

44
00:01:39,892 --> 00:01:42,350
After we import the data set,
the first thing we want to do

45
00:01:42,350 --> 00:01:44,589
is partition it
into Train and Test.

46
00:01:44,590 --> 00:01:46,640
And to do that, we can
import a handy utility,

47
00:01:46,640 --> 00:01:48,530
and it makes the syntax clear.

48
00:01:48,530 --> 00:01:50,340
We're taking our
x's and our y's,

49
00:01:50,340 --> 00:01:52,930
or our features and labels,
and partitioning them

50
00:01:52,930 --> 00:01:54,450
into two sets.

51
00:01:54,450 --> 00:01:56,690
X_train and y_train are
the features and labels

52
00:01:56,690 --> 00:01:57,980
for the training set.

53
00:01:57,980 --> 00:02:00,630
And X_test and y_test are
the features and labels

54
00:02:00,630 --> 00:02:02,032
for the testing set.

55
00:02:02,032 --> 00:02:04,240
Here, I'm just saying that
I want half the data to be

56
00:02:04,240 --> 00:02:05,580
used for testing.

57
00:02:05,580 --> 00:02:09,228
So if we have 150 examples
in Iris, 75 will be in Train

58
00:02:09,229 --> 00:02:11,520
and 75 will be in Test.

59
00:02:11,520 --> 00:02:13,280
Now we'll create our classifier.

60
00:02:13,280 --> 00:02:14,980
I'll use two
different types here

61
00:02:14,980 --> 00:02:17,859
to show you how they
accomplish the same task.

62
00:02:17,860 --> 00:02:20,500
Let's start with the decision
tree we've already seen.

63
00:02:20,500 --> 00:02:22,240
Note there's only
two lines of code

64
00:02:22,240 --> 00:02:23,448
that are classifier-specific.

65
00:02:23,448 --> 00:02:25,650


66
00:02:25,650 --> 00:02:28,830
Now let's train the classifier
using our training data.

67
00:02:28,830 --> 00:02:31,600
At this point, it's ready
to be used to classify data.

68
00:02:31,600 --> 00:02:33,329
And next, we'll call
the predict method

69
00:02:33,330 --> 00:02:35,805
and use it to classify
our testing data.

70
00:02:35,805 --> 00:02:37,180
If you print out
the predictions,

71
00:02:37,180 --> 00:02:38,970
you'll see there are
a list of numbers.

72
00:02:38,970 --> 00:02:40,660
These correspond
to the type of Iris

73
00:02:40,660 --> 00:02:44,010
the classifier predicts for
each row in the testing data.

74
00:02:44,010 --> 00:02:46,230
Now let's see how
accurate our classifier

75
00:02:46,230 --> 00:02:48,280
was on the testing set.

76
00:02:48,280 --> 00:02:50,840
Recall that up top, we have
the true labels for the testing

77
00:02:50,840 --> 00:02:51,650
data.

78
00:02:51,650 --> 00:02:53,460
To calculate our
accuracy, we can

79
00:02:53,460 --> 00:02:55,760
compare the predicted
labels to the true labels,

80
00:02:55,760 --> 00:02:57,349
and tally up the score.

81
00:02:57,349 --> 00:02:59,140
There's a convenience
method in [? Sykit ?]

82
00:02:59,140 --> 00:03:00,829
we can import to do that.

83
00:03:00,830 --> 00:03:03,505
Notice here, our
accuracy was over 90%.

84
00:03:03,505 --> 00:03:06,130
If you try this on your own, it
might be a little bit different

85
00:03:06,130 --> 00:03:08,269
because of some randomness
in how the Train/Test

86
00:03:08,270 --> 00:03:10,040
data is partitioned.

87
00:03:10,040 --> 00:03:11,880
Now, here's something
interesting.

88
00:03:11,880 --> 00:03:14,690
By replacing these two lines, we
can use a different classifier

89
00:03:14,690 --> 00:03:16,920
to accomplish the same task.

90
00:03:16,920 --> 00:03:18,570
Instead of using
a decision tree,

91
00:03:18,570 --> 00:03:20,930
we'll use one called
[? KNearestNeighbors. ?]

92
00:03:20,930 --> 00:03:23,340
If we run our experiment,
we'll see that the code

93
00:03:23,340 --> 00:03:25,354
works in exactly the same way.

94
00:03:25,354 --> 00:03:27,270
The accuracy may be
different when you run it,

95
00:03:27,270 --> 00:03:29,800
because this classifier works
a little bit differently

96
00:03:29,800 --> 00:03:32,440
and because of the randomness
in the Train/Test split.

97
00:03:32,440 --> 00:03:35,420
Likewise, if we wanted to use a
more sophisticated classifier,

98
00:03:35,420 --> 00:03:38,220
we could just import it
and change these two lines.

99
00:03:38,220 --> 00:03:40,297
Otherwise, our code is the same.

100
00:03:40,297 --> 00:03:42,880
The takeaway here is that while
there are many different types

101
00:03:42,880 --> 00:03:45,920
of classifiers, at a high level,
they have a similar interface.

102
00:03:45,920 --> 00:03:49,059


103
00:03:49,059 --> 00:03:50,850
Now let's talk a little
bit more about what

104
00:03:50,850 --> 00:03:53,120
it means to learn from data.

105
00:03:53,120 --> 00:03:56,080
Earlier, I said we called the
features x and the labels y,

106
00:03:56,080 --> 00:03:58,717
because they were the input
and output of a function.

107
00:03:58,717 --> 00:04:00,800
Now, of course, a function
is something we already

108
00:04:00,800 --> 00:04:02,190
know from programming.

109
00:04:02,190 --> 00:04:04,900
def classify--
there's our function.

110
00:04:04,900 --> 00:04:06,920
As we already know in
supervised learning,

111
00:04:06,920 --> 00:04:09,059
we don't want to
write this ourselves.

112
00:04:09,060 --> 00:04:12,360
We want an algorithm to
learn it from training data.

113
00:04:12,360 --> 00:04:15,240
So what does it mean
to learn a function?

114
00:04:15,240 --> 00:04:17,120
Well, a function is just
a mapping from input

115
00:04:17,120 --> 00:04:18,660
to output values.

116
00:04:18,660 --> 00:04:20,660
Here's a function you
might have seen before-- y

117
00:04:20,660 --> 00:04:22,700
equals mx plus b.

118
00:04:22,700 --> 00:04:24,820
That's the equation
for a line, and there

119
00:04:24,820 --> 00:04:27,340
are two parameters-- m,
which gives the slope;

120
00:04:27,340 --> 00:04:29,679
and b, which gives
the y-intercept.

121
00:04:29,680 --> 00:04:31,110
Given these
parameters, of course,

122
00:04:31,110 --> 00:04:34,320
we can plot the function
for different values of x.

123
00:04:34,320 --> 00:04:36,610
Now, in supervised learning,
our classified function

124
00:04:36,610 --> 00:04:38,420
might have some
parameters as well,

125
00:04:38,420 --> 00:04:41,290
but the input x are the
features for an example we

126
00:04:41,290 --> 00:04:43,630
want to classify,
and the output y

127
00:04:43,630 --> 00:04:47,219
is a label, like Spam or Not
Spam, or a type of flower.

128
00:04:47,220 --> 00:04:49,661
So what could the body of
the function look like?

129
00:04:49,661 --> 00:04:51,910
Well, that's the part we
want to write algorithmically

130
00:04:51,910 --> 00:04:53,737
or in other words, learn.

131
00:04:53,737 --> 00:04:55,320
The important thing
to understand here

132
00:04:55,320 --> 00:04:57,130
is we're not
starting from scratch

133
00:04:57,130 --> 00:05:00,060
and pulling the body of the
function out of thin air.

134
00:05:00,060 --> 00:05:01,990
Instead, we start with a model.

135
00:05:01,990 --> 00:05:04,050
And you can think of a
model as the prototype for

136
00:05:04,050 --> 00:05:07,030
or the rules that define
the body of our function.

137
00:05:07,030 --> 00:05:08,539
Typically, a model
has parameters

138
00:05:08,540 --> 00:05:10,290
that we can adjust
with our training data.

139
00:05:10,290 --> 00:05:14,560
And here's a high-level example
of how this process works.

140
00:05:14,560 --> 00:05:17,380
Let's look at a toy data set and
think about what kind of model

141
00:05:17,380 --> 00:05:19,210
we could use as a classifier.

142
00:05:19,210 --> 00:05:20,960
Pretend we're interested
in distinguishing

143
00:05:20,960 --> 00:05:23,349
between red dots and
green dots, some of which

144
00:05:23,350 --> 00:05:25,080
I've drawn here on a graph.

145
00:05:25,080 --> 00:05:27,210
To do that, we'll use
just two features--

146
00:05:27,210 --> 00:05:29,450
the x- and
y-coordinates of a dot.

147
00:05:29,450 --> 00:05:32,670
Now let's think about how
we could classify this data.

148
00:05:32,670 --> 00:05:34,090
We want a function
that considers

149
00:05:34,090 --> 00:05:35,799
a new dot it's
never seen before,

150
00:05:35,800 --> 00:05:38,170
and classifies it
as red or green.

151
00:05:38,170 --> 00:05:40,990
In fact, there might be a lot
of data we want to classify.

152
00:05:40,990 --> 00:05:42,840
Here, I've drawn
our testing examples

153
00:05:42,840 --> 00:05:44,960
in light green and light red.

154
00:05:44,960 --> 00:05:47,210
These are dots that weren't
in our training data.

155
00:05:47,210 --> 00:05:49,789
The classifier has never
seen them before, so how can

156
00:05:49,790 --> 00:05:51,700
it predict the right label?

157
00:05:51,700 --> 00:05:53,820
Well, imagine if we
could somehow draw a line

158
00:05:53,820 --> 00:05:56,037
across the data like this.

159
00:05:56,037 --> 00:05:57,620
Then we could say
the dots to the left

160
00:05:57,620 --> 00:06:00,090
of the line are green and dots
to the right of the line are

161
00:06:00,090 --> 00:06:00,919
red.

162
00:06:00,920 --> 00:06:03,430
And this line can serve
as our classifier.

163
00:06:03,430 --> 00:06:05,610
So how can we learn this line?

164
00:06:05,610 --> 00:06:08,240
Well, one way is to use
the training data to adjust

165
00:06:08,240 --> 00:06:09,880
the parameters of a model.

166
00:06:09,880 --> 00:06:12,830
And let's say the model we
use is a simple straight line

167
00:06:12,830 --> 00:06:14,460
like we saw before.

168
00:06:14,460 --> 00:06:17,830
That means we have two
parameters to adjust-- m and b.

169
00:06:17,830 --> 00:06:21,050
And by changing them, we can
change where the line appears.

170
00:06:21,050 --> 00:06:23,500
So how could we learn
the right parameters?

171
00:06:23,500 --> 00:06:25,690
Well, one idea is that
we can iteratively adjust

172
00:06:25,690 --> 00:06:27,640
them using our training data.

173
00:06:27,640 --> 00:06:29,890
For example, we might
start with a random line

174
00:06:29,890 --> 00:06:32,810
and use it to classify the
first training example.

175
00:06:32,810 --> 00:06:35,370
If it gets it right, we don't
need to change our line,

176
00:06:35,370 --> 00:06:36,969
so we move on to the next one.

177
00:06:36,969 --> 00:06:38,760
But on the other hand,
if it gets it wrong,

178
00:06:38,760 --> 00:06:41,300
we could slightly adjust
the parameters of our model

179
00:06:41,300 --> 00:06:43,070
to make it more accurate.

180
00:06:43,070 --> 00:06:44,680
The takeaway here is this.

181
00:06:44,680 --> 00:06:47,490
One way to think of learning
is using training data

182
00:06:47,490 --> 00:06:50,980
to adjust the
parameters of a model.

183
00:06:50,980 --> 00:06:52,950
Now, here's something
really special.

184
00:06:52,950 --> 00:06:55,270
It's called
tensorflow/playground.

185
00:06:55,270 --> 00:06:57,370
This is a beautiful
example of a neural network

186
00:06:57,370 --> 00:07:00,020
you can run and experiment
with right in your browser.

187
00:07:00,020 --> 00:07:02,060
Now, this deserves its
own episode for sure,

188
00:07:02,060 --> 00:07:03,730
but for now, go ahead
and play with it.

189
00:07:03,730 --> 00:07:04,930
It's awesome.

190
00:07:04,930 --> 00:07:06,630
The playground comes
with different data

191
00:07:06,630 --> 00:07:08,300
sets you can try out.

192
00:07:08,300 --> 00:07:09,470
Some are very simple.

193
00:07:09,470 --> 00:07:12,620
For example, we could use our
line to classify this one.

194
00:07:12,620 --> 00:07:15,980
Some data sets are
much more complex.

195
00:07:15,980 --> 00:07:17,620
This data set is
especially hard.

196
00:07:17,620 --> 00:07:20,357
And see if you can build
a network to classify it.

197
00:07:20,357 --> 00:07:21,940
Now, you can think
of a neural network

198
00:07:21,940 --> 00:07:24,170
as a more sophisticated
type of classifier,

199
00:07:24,170 --> 00:07:26,430
like a decision tree
or a simple line.

200
00:07:26,430 --> 00:07:29,191
But in principle,
the idea is similar.

201
00:07:29,191 --> 00:07:29,690
OK.

202
00:07:29,690 --> 00:07:30,687
Hope that was helpful.

203
00:07:30,687 --> 00:07:32,520
I just created a Twitter
that you can follow

204
00:07:32,520 --> 00:07:33,834
to be notified of new episodes.

205
00:07:33,834 --> 00:07:36,000
And the next one should be
out in a couple of weeks,

206
00:07:36,000 --> 00:07:38,750
depending on how much work I'm
doing for Google I/O. Thanks,

207
00:07:38,750 --> 00:07:41,620
as always, for watching,
and I'll see you next time.

208
00:07:41,620 --> 00:07:53,871


