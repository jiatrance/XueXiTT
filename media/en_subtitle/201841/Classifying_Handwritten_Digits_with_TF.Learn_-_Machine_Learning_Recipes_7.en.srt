1
00:00:00,000 --> 00:00:06,011


2
00:00:06,011 --> 00:00:07,510
JOSH GORDON: Last
episode we trained

3
00:00:07,510 --> 00:00:10,500
in Image Classifier using
TensorFlow for Poets,

4
00:00:10,500 --> 00:00:13,552
and this time, we'll
write one using TF.Learn.

5
00:00:13,552 --> 00:00:15,010
The problem we'll
start on today is

6
00:00:15,010 --> 00:00:18,400
classifying handwritten
digits from the MNIST dataset,

7
00:00:18,400 --> 00:00:21,000
and writing a simple
classifier for these is often

8
00:00:21,000 --> 00:00:23,780
considered the Hello
World of computer vision.

9
00:00:23,780 --> 00:00:26,940
Now MNIST is a multi-class
classification problem.

10
00:00:26,940 --> 00:00:28,620
Given an image of
a digit, our job

11
00:00:28,620 --> 00:00:30,880
will be to predict
which one it is.

12
00:00:30,880 --> 00:00:32,900
I wrote an IPython
notebook for this episode,

13
00:00:32,900 --> 00:00:35,110
and you can find a link
to it in the description.

14
00:00:35,110 --> 00:00:37,630
And to make it easier for you
to configure your environment,

15
00:00:37,630 --> 00:00:39,630
I'll start with a quick
screencast of installing

16
00:00:39,630 --> 00:00:41,576
TensorFlow using Docker.

17
00:00:41,576 --> 00:00:43,450
First, here's an outline
of what we'll cover.

18
00:00:43,450 --> 00:00:45,158
I'll show you how to
download the dataset

19
00:00:45,158 --> 00:00:46,530
and visualize images.

20
00:00:46,530 --> 00:00:49,050
Next, we'll train a
classifier, evaluate it,

21
00:00:49,050 --> 00:00:51,910
and use it to make
predictions on new images.

22
00:00:51,911 --> 00:00:54,160
Then we'll visualize the
weights the classifier learns

23
00:00:54,160 --> 00:00:56,940
to gain intuition for how
it works under the hood.

24
00:00:56,940 --> 00:00:59,397
Let's start by
installing TensorFlow.

25
00:00:59,397 --> 00:01:00,980
You can find
installation instructions

26
00:01:00,980 --> 00:01:03,089
for Docker linked from
the Getting Started page

27
00:01:03,090 --> 00:01:06,020
on TensorFlow.org, and
I'll start this screencast

28
00:01:06,020 --> 00:01:08,479
assuming you've just finished
downloading and installing

29
00:01:08,480 --> 00:01:12,450
Docker itself but haven't
started installing TensorFlow.

30
00:01:12,450 --> 00:01:15,300
Starting from a fresh install
of Docker, the first thing to do

31
00:01:15,300 --> 00:01:17,539
is open the Docker
Quickstart terminal.

32
00:01:17,540 --> 00:01:20,040
And when this appears,
you'll see an IP address just

33
00:01:20,040 --> 00:01:21,280
below the whale.

34
00:01:21,280 --> 00:01:22,060
Copy it down.

35
00:01:22,060 --> 00:01:23,509
We'll need it later.

36
00:01:23,509 --> 00:01:25,050
Next, we'll launch
a Docker container

37
00:01:25,050 --> 00:01:26,810
with a TensorFlow image.

38
00:01:26,810 --> 00:01:28,670
The image is hosted
on Docker hub,

39
00:01:28,670 --> 00:01:30,810
and there's a link to
that in the description.

40
00:01:30,810 --> 00:01:33,580
The image contains TensorFlow
with all its dependencies

41
00:01:33,580 --> 00:01:35,880
properly configured,
and here's the command

42
00:01:35,880 --> 00:01:38,360
we'll use to download
and launch the image.

43
00:01:38,360 --> 00:01:41,574
But first, let's choose
the version we want.

44
00:01:41,574 --> 00:01:43,240
The versions are on
this page, and we'll

45
00:01:43,240 --> 00:01:44,850
use the latest release.

46
00:01:44,850 --> 00:01:47,419
Now we can copy-paste the
command into a terminal

47
00:01:47,420 --> 00:01:49,670
and add a colon with
the version number.

48
00:01:49,670 --> 00:01:51,700
If this is the first time
you've run the image,

49
00:01:51,700 --> 00:01:53,260
it'll be downloaded
automatically.

50
00:01:53,260 --> 00:01:56,257
And on subsequent runs,
it'll be cached locally.

51
00:01:56,257 --> 00:01:58,340
The image starts automatically,
and by default, it

52
00:01:58,340 --> 00:01:59,500
runs a notebook server.

53
00:01:59,500 --> 00:02:01,880
All that's left for us to
do is to open up a browser

54
00:02:01,880 --> 00:02:06,310
and point it to the IP we jotted
down earlier on port 8888.

55
00:02:06,310 --> 00:02:08,060
And now we have an
IPython notebook

56
00:02:08,060 --> 00:02:10,430
that we can experiment
with in our browser served

57
00:02:10,430 --> 00:02:11,549
by the container.

58
00:02:11,549 --> 00:02:14,090
You can find the notebook for
this episode in the description

59
00:02:14,090 --> 00:02:16,590
and upload it through the UI.

60
00:02:16,590 --> 00:02:17,090
OK.

61
00:02:17,090 --> 00:02:18,600
Now onto code.

62
00:02:18,600 --> 00:02:20,180
Here are the imports we'll use.

63
00:02:20,180 --> 00:02:22,670
I'll use matplotlib to display
images, and, of course,

64
00:02:22,670 --> 00:02:25,429
we'll use TF.Learn to
train the classifier.

65
00:02:25,430 --> 00:02:27,810
All of these are
installed with the image.

66
00:02:27,810 --> 00:02:30,200
Next, we'll download
the MNIST dataset,

67
00:02:30,200 --> 00:02:32,709
and we have a nice
one liner for that.

68
00:02:32,710 --> 00:02:35,080
The dataset contains
thousands of labeled images

69
00:02:35,080 --> 00:02:36,450
of handwritten digits.

70
00:02:36,450 --> 00:02:39,459
It's pre-divided into
train, which is 55,000,

71
00:02:39,460 --> 00:02:41,620
and test, which is 10,000.

72
00:02:41,620 --> 00:02:44,520
Let's visualize a few
of these to get a feel.

73
00:02:44,520 --> 00:02:47,380
This code displays an
image along with its label,

74
00:02:47,380 --> 00:02:49,280
and you might notice
I'm reshaping the image,

75
00:02:49,280 --> 00:02:52,050
and I'll explain why in a bit.

76
00:02:52,050 --> 00:02:54,480
The first image from the
testing set is a seven,

77
00:02:54,480 --> 00:02:57,760
and you can see the example
index as well as the label.

78
00:02:57,760 --> 00:02:59,660
Here's the second image.

79
00:02:59,660 --> 00:03:01,180
Now both of these
are clearly drawn,

80
00:03:01,180 --> 00:03:03,096
but there's a variety
of different handwriting

81
00:03:03,096 --> 00:03:04,300
samples in this dataset.

82
00:03:04,300 --> 00:03:06,990
Here's an image that's
harder to recognize.

83
00:03:06,990 --> 00:03:09,340
These images are low
resolution, just 28

84
00:03:09,340 --> 00:03:11,750
by 28 pixels in grayscale.

85
00:03:11,750 --> 00:03:14,250
Also note they're
properly segmented.

86
00:03:14,250 --> 00:03:17,760
That means each image
contains exactly one digit.

87
00:03:17,760 --> 00:03:20,049
Now let's talk about
the features we'll use.

88
00:03:20,050 --> 00:03:21,660
When we're working
with images, we

89
00:03:21,660 --> 00:03:23,770
use the raw pixels as features.

90
00:03:23,770 --> 00:03:25,560
That's because extracting
useful features

91
00:03:25,560 --> 00:03:28,360
from images, like textures
and shapes, is hard.

92
00:03:28,360 --> 00:03:31,770
Now a 28 by 28 image
has 784 pixels,

93
00:03:31,770 --> 00:03:34,117
so we have 784 features.

94
00:03:34,117 --> 00:03:36,200
And here, we're using the
flattened representation

95
00:03:36,200 --> 00:03:37,560
of the image.

96
00:03:37,560 --> 00:03:40,250
To flatten an image means to
convert it from a 2D array

97
00:03:40,250 --> 00:03:43,566
to a 1D array by unstacking
the rows and lining them up.

98
00:03:43,566 --> 00:03:45,190
That's why we had to
reshape this array

99
00:03:45,190 --> 00:03:47,260
to display it earlier.

100
00:03:47,260 --> 00:03:49,200
Now we can initialize
the classifier,

101
00:03:49,200 --> 00:03:51,589
and here, we'll use
a linear classifier.

102
00:03:51,590 --> 00:03:53,360
We'll provide two parameters.

103
00:03:53,360 --> 00:03:55,730
The first indicates how
many classes we have,

104
00:03:55,730 --> 00:03:58,660
and there are 10, one
for each type of digit.

105
00:03:58,660 --> 00:04:00,230
The second informs
the classifier

106
00:04:00,230 --> 00:04:02,269
about the features we'll use.

107
00:04:02,270 --> 00:04:04,730
Now I'll draw a quick diagram
of a linear classifier

108
00:04:04,730 --> 00:04:07,595
to give you a high level preview
of how it works under the hood.

109
00:04:07,595 --> 00:04:08,970
You could think
of the classifier

110
00:04:08,970 --> 00:04:11,280
as adding up the evidence
that the image is

111
00:04:11,280 --> 00:04:13,020
each type of digit.

112
00:04:13,020 --> 00:04:15,910
The input nodes are on the
top, represented by Xes,

113
00:04:15,910 --> 00:04:19,219
and the output nodes are on
the bottom represented by Ys.

114
00:04:19,220 --> 00:04:22,750
We have one input node for each
feature or pixel in the image

115
00:04:22,750 --> 00:04:24,340
and one output
node for each digit

116
00:04:24,340 --> 00:04:26,099
the image could represent.

117
00:04:26,100 --> 00:04:29,777
Here, we have 784
inputs and 10 outputs.

118
00:04:29,777 --> 00:04:31,610
I've just drawn a few
of them, so everything

119
00:04:31,610 --> 00:04:32,740
fits on the screen.

120
00:04:32,740 --> 00:04:35,240
Now the inputs and outputs
are fully connected,

121
00:04:35,240 --> 00:04:37,560
and each of these
edges has a weight.

122
00:04:37,560 --> 00:04:40,350
When we classify an image,
you can think of each pixel

123
00:04:40,350 --> 00:04:41,830
as going on a journey.

124
00:04:41,830 --> 00:04:44,130
First, it flows
into its input node,

125
00:04:44,130 --> 00:04:46,650
and next, it travels
along the edges.

126
00:04:46,650 --> 00:04:49,590
Along the way, it's multiplied
by the weight on the edge,

127
00:04:49,590 --> 00:04:51,650
and the output nodes
gather evidence

128
00:04:51,650 --> 00:04:55,750
that the image we're classifying
represents each type of digit.

129
00:04:55,750 --> 00:04:58,430
The more evidence we gather,
say on the eight output,

130
00:04:58,430 --> 00:05:01,060
the more likely it is
the image is an eight.

131
00:05:01,060 --> 00:05:03,160
And to calculate how
much evidence we have,

132
00:05:03,160 --> 00:05:06,250
we sum the value of the
pixel intensities multiplied

133
00:05:06,250 --> 00:05:07,430
by the weights.

134
00:05:07,430 --> 00:05:09,970
Then we can predict that the
image belongs to the output

135
00:05:09,970 --> 00:05:12,170
node with the most evidence.

136
00:05:12,170 --> 00:05:13,870
The important part
is the weights,

137
00:05:13,870 --> 00:05:15,420
and by setting them
properly, we can

138
00:05:15,420 --> 00:05:17,160
get accurate classifications.

139
00:05:17,160 --> 00:05:20,130
We begin with random weights,
then gradually adjust them

140
00:05:20,130 --> 00:05:21,510
towards better values.

141
00:05:21,510 --> 00:05:24,030
And this happens
inside the fit method.

142
00:05:24,030 --> 00:05:26,820
Once we have a trained
model, we can evaluate it.

143
00:05:26,820 --> 00:05:28,290
Using the evaluate
method, we see

144
00:05:28,290 --> 00:05:31,730
that it correctly classifies
about 90% of the test set.

145
00:05:31,730 --> 00:05:34,860
We can also make predictions
on individual images.

146
00:05:34,860 --> 00:05:37,400
Here's one that it correctly
classifies, and here's

147
00:05:37,400 --> 00:05:39,676
one that it gets wrong.

148
00:05:39,676 --> 00:05:41,800
Now I want to show you how
to visualize the weights

149
00:05:41,800 --> 00:05:43,520
the classifier learns.

150
00:05:43,520 --> 00:05:45,599
Here, positive weights
are drawn in red,

151
00:05:45,600 --> 00:05:48,650
and negative weights
are drawn in blue.

152
00:05:48,650 --> 00:05:50,440
So what do these
weights tell us?

153
00:05:50,440 --> 00:05:53,800
Well, to understand that,
I'll show four images of ones.

154
00:05:53,800 --> 00:05:55,480
They're all drawn
slightly differently,

155
00:05:55,480 --> 00:05:57,600
but take a look at
the middle pixel.

156
00:05:57,600 --> 00:06:00,390
Notice that it's filled
in on every image.

157
00:06:00,390 --> 00:06:02,289
When that pixel
is filled in, it's

158
00:06:02,290 --> 00:06:04,640
evidence that the image
we're looking at is a one,

159
00:06:04,640 --> 00:06:07,409
so we'd expect a
highway on that edge.

160
00:06:07,410 --> 00:06:09,270
Now let's take a
look at four zeros.

161
00:06:09,270 --> 00:06:11,450
Notice that the
middle pixel is empty.

162
00:06:11,450 --> 00:06:13,680
Although there's lots
of ways to draw zeros,

163
00:06:13,680 --> 00:06:15,330
if that middle
pixel is filled in,

164
00:06:15,330 --> 00:06:17,659
it's evidence against
the image being a zero,

165
00:06:17,660 --> 00:06:19,972
so we'd expect a negative
weight on the edge.

166
00:06:19,972 --> 00:06:21,680
And looking at the
images of the weights,

167
00:06:21,680 --> 00:06:23,940
we can almost see outlines
of the digits drawn

168
00:06:23,940 --> 00:06:25,935
in red for each class.

169
00:06:25,935 --> 00:06:28,060
We were able to visualize
these, because we started

170
00:06:28,060 --> 00:06:31,870
with 784 pixels, and we learned
10 weights for each, one

171
00:06:31,870 --> 00:06:33,350
for each type of digit.

172
00:06:33,350 --> 00:06:36,200
We then reshape the
weights into a 2D array.

173
00:06:36,200 --> 00:06:36,700
OK.

174
00:06:36,700 --> 00:06:37,489
That's it for now.

175
00:06:37,489 --> 00:06:39,530
Of course, there's lots
more to learn about this,

176
00:06:39,530 --> 00:06:41,614
and I put my favorite
links in the description.

177
00:06:41,615 --> 00:06:43,990
Coming up next time, we'll
experiment with deep learning,

178
00:06:43,990 --> 00:06:46,860
and I'll cover in more detail
what we introduced here today.

179
00:06:46,860 --> 00:06:49,760
Thanks very much for watching,
and I'll see you then.

180
00:06:49,760 --> 00:06:53,110
[MUSIC PLAYING]

181
00:06:53,110 --> 00:07:00,698


