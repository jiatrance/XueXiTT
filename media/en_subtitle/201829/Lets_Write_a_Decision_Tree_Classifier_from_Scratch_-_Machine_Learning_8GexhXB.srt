1
00:00:00,000 --> 00:00:05,486


2
00:00:05,486 --> 00:00:06,609
JOSH GORDON: Hey, everyone.

3
00:00:06,610 --> 00:00:07,709
Welcome back.

4
00:00:07,709 --> 00:00:10,000
In this episode, we'll write
a decision tree classifier

5
00:00:10,000 --> 00:00:12,560
from scratch in pure Python.

6
00:00:12,560 --> 00:00:14,420
Here's an outline
of what we'll cover.

7
00:00:14,420 --> 00:00:17,330
I'll start by introducing
the data set we'll work with.

8
00:00:17,330 --> 00:00:19,310
Next, we'll preview
the completed tree.

9
00:00:19,310 --> 00:00:20,450
And then, we'll build it.

10
00:00:20,450 --> 00:00:23,030
On the way, we'll cover concepts
like decision tree learning,

11
00:00:23,030 --> 00:00:24,880
Gini impurity, and
information gain.

12
00:00:24,880 --> 00:00:26,630
And you can find the
code for this episode

13
00:00:26,630 --> 00:00:27,560
in the description.

14
00:00:27,560 --> 00:00:29,599
And it's available
in two formats,

15
00:00:29,600 --> 00:00:32,990
both as a Jupiter notebook
and as a regular Python file.

16
00:00:32,990 --> 00:00:34,679
OK, let's get started.

17
00:00:34,679 --> 00:00:36,387
For this episode, I've
written a toy data

18
00:00:36,387 --> 00:00:39,590
set that includes both numeric
and categorical attributes.

19
00:00:39,590 --> 00:00:42,050
And here, our goal will be
to predict the type of fruit,

20
00:00:42,050 --> 00:00:44,360
like an apple or a
grape, based on features

21
00:00:44,360 --> 00:00:46,287
like color and size.

22
00:00:46,287 --> 00:00:47,870
At the end of the
episode, I encourage

23
00:00:47,870 --> 00:00:50,239
you to swap out this data
set for one of your own

24
00:00:50,240 --> 00:00:53,250
and build a tree for a
problem you care about.

25
00:00:53,250 --> 00:00:54,390
Let's look at the format.

26
00:00:54,390 --> 00:00:56,420
I've re-drawn it
here for clarity.

27
00:00:56,420 --> 00:00:57,620
Each row is an example.

28
00:00:57,620 --> 00:01:00,230
And the first two columns
provide features or attributes

29
00:01:00,230 --> 00:01:02,064
that describe the data.

30
00:01:02,064 --> 00:01:03,980
The last column gives
the label, or the class,

31
00:01:03,980 --> 00:01:05,229
we want to predict.

32
00:01:05,230 --> 00:01:07,310
And if you like, you
can modify this data set

33
00:01:07,310 --> 00:01:09,740
by adding additional
features or more examples,

34
00:01:09,740 --> 00:01:12,095
and our program will work
in exactly the same way.

35
00:01:12,095 --> 00:01:13,970
Now, this data set is
pretty straightforward,

36
00:01:13,970 --> 00:01:15,470
except for one thing.

37
00:01:15,470 --> 00:01:18,192
I've written it so it's
not perfectly separable.

38
00:01:18,192 --> 00:01:20,149
And by that I mean there's
no way to tell apart

39
00:01:20,150 --> 00:01:22,340
the second and fifth examples.

40
00:01:22,340 --> 00:01:24,650
They have the same features,
but different labels.

41
00:01:24,650 --> 00:01:27,555
And this is so we can see how
our tree handles this case.

42
00:01:27,555 --> 00:01:29,180
Towards the end of
the notebook, you'll

43
00:01:29,180 --> 00:01:31,040
find testing data
in the same format.

44
00:01:31,040 --> 00:01:33,200


45
00:01:33,200 --> 00:01:35,700
Now I've written a few utility
functions that make it easier

46
00:01:35,700 --> 00:01:36,947
to work with this data.

47
00:01:36,947 --> 00:01:39,030
And below each function,
I've written a small demo

48
00:01:39,030 --> 00:01:40,351
to show how it works.

49
00:01:40,351 --> 00:01:42,600
And I've repeated this pattern
for every block of code

50
00:01:42,600 --> 00:01:45,476
in the notebook.

51
00:01:45,477 --> 00:01:47,810
Now to build the tree, we use
the decision tree learning

52
00:01:47,810 --> 00:01:49,299
algorithm called CART.

53
00:01:49,299 --> 00:01:51,590
And as it happens, there's
a whole family of algorithms

54
00:01:51,590 --> 00:01:53,404
used to build trees from data.

55
00:01:53,404 --> 00:01:55,070
At their core, they
give you a procedure

56
00:01:55,070 --> 00:01:58,429
to decide which questions
to ask and when.

57
00:01:58,430 --> 00:02:01,190
CART stands for Classification
and Regression Trees.

58
00:02:01,190 --> 00:02:04,039
And here's a preview
of how it works.

59
00:02:04,040 --> 00:02:06,500
To begin, we'll add a
root node for the tree.

60
00:02:06,500 --> 00:02:09,080
And all nodes receive a
list of rows as input.

61
00:02:09,080 --> 00:02:12,090
And the root will receive
the entire training set.

62
00:02:12,090 --> 00:02:14,150
Now each node will ask
a true false question

63
00:02:14,150 --> 00:02:16,010
about one of the features.

64
00:02:16,010 --> 00:02:17,630
And in response
to this question,

65
00:02:17,630 --> 00:02:21,299
we split, or partition,
the data into two subsets.

66
00:02:21,300 --> 00:02:24,080
These subsets then become
the input to two child nodes

67
00:02:24,080 --> 00:02:25,970
we add to the tree.

68
00:02:25,970 --> 00:02:28,520
And the goal of the question
is to unmix the labels

69
00:02:28,520 --> 00:02:30,050
as we proceed down.

70
00:02:30,050 --> 00:02:32,600
Or in other words, to
produce the purest possible

71
00:02:32,600 --> 00:02:35,552
distribution of the
labels at each node.

72
00:02:35,552 --> 00:02:37,010
For example, the
input to this node

73
00:02:37,010 --> 00:02:39,410
contains only a
single type of label,

74
00:02:39,410 --> 00:02:41,570
so we'd say it's
perfectly unmixed.

75
00:02:41,570 --> 00:02:44,630
There's no uncertainty
about the type of label.

76
00:02:44,630 --> 00:02:47,540
On the other hand, the labels
in this node are still mixed up,

77
00:02:47,540 --> 00:02:50,510
so we'd ask another question
to further narrow it down.

78
00:02:50,510 --> 00:02:52,399
And the trick to building
an effective tree

79
00:02:52,400 --> 00:02:55,590
is to understand which
questions to ask and when.

80
00:02:55,590 --> 00:02:58,340
And to do that, we need to
quantify how much a question

81
00:02:58,340 --> 00:03:00,216
helps to unmix the labels.

82
00:03:00,216 --> 00:03:02,090
And we can quantify the
amount of uncertainty

83
00:03:02,090 --> 00:03:05,176
at a single node using a
metric called Gini impurity.

84
00:03:05,176 --> 00:03:06,799
And we can quantify
how much a question

85
00:03:06,800 --> 00:03:09,140
reduces that uncertainty
using a concept

86
00:03:09,140 --> 00:03:11,100
called information gain.

87
00:03:11,100 --> 00:03:12,769
We'll use these
to select the best

88
00:03:12,770 --> 00:03:14,540
question to ask at each point.

89
00:03:14,540 --> 00:03:17,150
And given that question, we'll
recursively build the tree

90
00:03:17,150 --> 00:03:18,840
on each of the new nodes.

91
00:03:18,840 --> 00:03:20,840
We'll continue dividing
the data until there

92
00:03:20,840 --> 00:03:23,330
are no further questions
to ask, at which point

93
00:03:23,330 --> 00:03:24,680
we'll add a leaf.

94
00:03:24,680 --> 00:03:26,630
To implement this, first
we need to understand

95
00:03:26,630 --> 00:03:29,120
what type of questions
can we ask about the data.

96
00:03:29,120 --> 00:03:30,680
And second, we
need to understand

97
00:03:30,680 --> 00:03:32,770
how to decide which
question to ask when.

98
00:03:32,770 --> 00:03:35,520


99
00:03:35,520 --> 00:03:37,980
Now each node takes a
list of rows as input.

100
00:03:37,980 --> 00:03:39,660
And to generate a
list of questions

101
00:03:39,660 --> 00:03:42,359
we'll iterate over every
value for every feature that

102
00:03:42,360 --> 00:03:44,166
appears in those rows.

103
00:03:44,166 --> 00:03:45,540
Each of these
becomes a candidate

104
00:03:45,540 --> 00:03:48,239
for a threshold we can
use to partition the data.

105
00:03:48,240 --> 00:03:51,120
And there will often
be many possibilities.

106
00:03:51,120 --> 00:03:52,950
In code we represent
a question by storing

107
00:03:52,950 --> 00:03:55,170
a column number
and a column value,

108
00:03:55,170 --> 00:03:58,035
or the threshold we'll
use to partition the data.

109
00:03:58,035 --> 00:03:59,910
For example, here's how
we'd write a question

110
00:03:59,910 --> 00:04:01,850
to test if the color is green.

111
00:04:01,850 --> 00:04:03,930
And here's an example
for a numeric attribute

112
00:04:03,930 --> 00:04:07,620
to test if the diameter is
greater than or equal to 3.

113
00:04:07,620 --> 00:04:10,320
In response to a question, we
divide, or partition, the data

114
00:04:10,320 --> 00:04:12,060
into two subsets.

115
00:04:12,060 --> 00:04:14,880
The first contains all the rows
for which the question is true.

116
00:04:14,880 --> 00:04:17,610
And the second contains
everything else.

117
00:04:17,610 --> 00:04:19,740
In code, our partition
function takes a question

118
00:04:19,740 --> 00:04:21,720
and a list of rows as input.

119
00:04:21,720 --> 00:04:24,060
For example, here's how we
partition the rows based

120
00:04:24,060 --> 00:04:26,950
on whether the color is red.

121
00:04:26,950 --> 00:04:29,214
Here, true rows contains
all the red examples.

122
00:04:29,214 --> 00:04:30,880
And false rows contains
everything else.

123
00:04:30,880 --> 00:04:33,885


124
00:04:33,886 --> 00:04:36,260
The best question is the one
that reduces our uncertainty

125
00:04:36,260 --> 00:04:37,219
the most.

126
00:04:37,220 --> 00:04:39,800
And Gini impurity let's us
quantify how much uncertainty

127
00:04:39,800 --> 00:04:41,180
there is at a node.

128
00:04:41,180 --> 00:04:43,190
Information gain will
let us quantify how much

129
00:04:43,190 --> 00:04:44,960
a question reduces that.

130
00:04:44,960 --> 00:04:46,880
Let's work on impurity first.

131
00:04:46,880 --> 00:04:49,490
Now this is a metric that
ranges between 0 and 1

132
00:04:49,490 --> 00:04:52,190
where lower values indicate
less uncertainty, or mixing,

133
00:04:52,190 --> 00:04:53,540
at a node.

134
00:04:53,540 --> 00:04:56,480
It quantifies our chance of
being incorrect if we randomly

135
00:04:56,480 --> 00:05:00,560
assign a label from a set
to an example in that set.

136
00:05:00,560 --> 00:05:02,940
Here's an example
to make that clear.

137
00:05:02,940 --> 00:05:06,240
Imagine we have two bowls
and one contains the examples

138
00:05:06,240 --> 00:05:08,430
and the other contains labels.

139
00:05:08,430 --> 00:05:11,760
First, we'll randomly draw an
example from the first bowl.

140
00:05:11,760 --> 00:05:14,130
Then we'll randomly draw
a label from the second.

141
00:05:14,130 --> 00:05:17,580
And now, we'll classify the
example as having that label.

142
00:05:17,580 --> 00:05:21,330
And Gini impurity gives us
our chance of being incorrect.

143
00:05:21,330 --> 00:05:23,969
In this example, we have
only apples in each bowl.

144
00:05:23,970 --> 00:05:25,480
There's no way to
make a mistake.

145
00:05:25,480 --> 00:05:28,335
So we say the impurity is zero.

146
00:05:28,335 --> 00:05:30,710
On the other hand, given a
bowl with five different types

147
00:05:30,710 --> 00:05:32,590
of fruit in equal
proportion, we'd

148
00:05:32,590 --> 00:05:35,119
say it has an impurity of 0.8.

149
00:05:35,120 --> 00:05:37,790
That's because we have a one out
of five chance of being right

150
00:05:37,790 --> 00:05:41,650
if we randomly assign
a label to an example.

151
00:05:41,650 --> 00:05:44,474
In code, this method calculates
the impurity of a data set.

152
00:05:44,474 --> 00:05:45,890
And I've written
a couple examples

153
00:05:45,890 --> 00:05:48,110
below that demonstrate
how it works.

154
00:05:48,110 --> 00:05:50,360
You can see the impurity
for the first set is zero

155
00:05:50,360 --> 00:05:51,770
because there's no mixing.

156
00:05:51,770 --> 00:05:57,010
And here, you can see
the impurity is 0.8.

157
00:05:57,010 --> 00:05:59,890
Now information gain will let us
find the question that reduces

158
00:05:59,890 --> 00:06:01,318
our uncertainty the most.

159
00:06:01,319 --> 00:06:02,860
And it's just a
number that describes

160
00:06:02,860 --> 00:06:06,340
how much a question helps to
unmix the labels at a node.

161
00:06:06,340 --> 00:06:07,909
Here's the idea.

162
00:06:07,910 --> 00:06:09,580
We begin by calculating
the uncertainty

163
00:06:09,580 --> 00:06:11,229
of our starting set.

164
00:06:11,230 --> 00:06:12,750
Then, for each
question we can ask,

165
00:06:12,750 --> 00:06:15,160
we'll try partitioning
the data and calculating

166
00:06:15,160 --> 00:06:18,087
the uncertainty of the
child nodes that result.

167
00:06:18,087 --> 00:06:20,170
We'll take a weighted
average of their uncertainty

168
00:06:20,170 --> 00:06:22,870
because we care more about a
large set with low uncertainty

169
00:06:22,870 --> 00:06:25,667
than a small set with high.

170
00:06:25,667 --> 00:06:28,000
Then, we'll subtract this
from our starting uncertainty.

171
00:06:28,000 --> 00:06:29,722
And that's our information gain.

172
00:06:29,722 --> 00:06:31,680
As we go, we'll keep
track of the question that

173
00:06:31,680 --> 00:06:33,300
produces the most gain.

174
00:06:33,300 --> 00:06:36,570
And that will be the best
one to ask at this node.

175
00:06:36,570 --> 00:06:38,460
Let's see how this
looks in code.

176
00:06:38,460 --> 00:06:41,409
Here, we'll iterate over
every value for the features.

177
00:06:41,409 --> 00:06:43,200
We'll generate a question
for that feature,

178
00:06:43,200 --> 00:06:45,750
then partition the data on it.

179
00:06:45,750 --> 00:06:49,259
Notice we discard any questions
that fail to produce a split.

180
00:06:49,259 --> 00:06:51,050
Then, we'll calculate
our information gain.

181
00:06:51,050 --> 00:06:52,424
And inside this
function, you can

182
00:06:52,424 --> 00:06:56,107
see we take a weighted average
and the impurity of each set.

183
00:06:56,107 --> 00:06:57,940
We see how much this
reduces the uncertainty

184
00:06:57,940 --> 00:06:59,350
from our starting set.

185
00:06:59,350 --> 00:07:01,850
And we keep track
of the best value.

186
00:07:01,850 --> 00:07:04,510
I've written a couple
of demos below as well.

187
00:07:04,510 --> 00:07:07,560
OK, with these concepts in hand,
we're ready to build the tree.

188
00:07:07,560 --> 00:07:10,060
And to put this all together I
think the most useful thing I

189
00:07:10,060 --> 00:07:11,890
can do is walk you
through the algorithm

190
00:07:11,890 --> 00:07:14,200
as it builds a tree
for our training data.

191
00:07:14,200 --> 00:07:17,770
This uses recursion, so seeing
it in action can be helpful.

192
00:07:17,770 --> 00:07:21,490
You can find the code for this
inside the Build Tree function.

193
00:07:21,490 --> 00:07:23,350
When we call build tree
for the first time,

194
00:07:23,350 --> 00:07:26,050
it receives the entire
training set as input.

195
00:07:26,050 --> 00:07:27,940
And as output it will
return a reference

196
00:07:27,940 --> 00:07:30,340
to the root node of our tree.

197
00:07:30,340 --> 00:07:33,039
I'll draw a placeholder
for the root here in gray.

198
00:07:33,040 --> 00:07:35,280
And here are the rows we're
considering at this node.

199
00:07:35,280 --> 00:07:38,380
And to start, that's
the entire training set.

200
00:07:38,380 --> 00:07:40,960
Now we find the best
question to ask at this node.

201
00:07:40,960 --> 00:07:44,810
And we do that by iterating
over each of these values.

202
00:07:44,810 --> 00:07:47,180
We'll split the data and
calculate the information

203
00:07:47,180 --> 00:07:48,560
gained for each one.

204
00:07:48,560 --> 00:07:50,690
And as we go, we'll keep
track of the question that

205
00:07:50,690 --> 00:07:53,790
produces the most gain.

206
00:07:53,790 --> 00:07:55,980
Now in this case, there's
a useful question to ask,

207
00:07:55,980 --> 00:07:57,960
so the gain will be
greater than zero.

208
00:07:57,960 --> 00:08:00,750
And we'll split the data
using that question.

209
00:08:00,750 --> 00:08:03,780
And now, we'll use recursion
by calling build tree again

210
00:08:03,780 --> 00:08:06,479
to add a node for
the true branch.

211
00:08:06,479 --> 00:08:08,520
The rows we're considering
now are the first half

212
00:08:08,520 --> 00:08:09,570
of the split.

213
00:08:09,570 --> 00:08:13,170
And again, we'll find the best
question to ask for this data.

214
00:08:13,170 --> 00:08:15,870
Once more we split and call
the build tree function

215
00:08:15,870 --> 00:08:17,910
to add the child node.

216
00:08:17,910 --> 00:08:20,400
Now for this data there are
no further questions to ask.

217
00:08:20,400 --> 00:08:22,289
So the information
gain will be zero.

218
00:08:22,290 --> 00:08:24,562
And this node becomes a leaf.

219
00:08:24,562 --> 00:08:26,270
It will predict that
an example is either

220
00:08:26,270 --> 00:08:28,820
an apple or a lemon
with 50% confidence

221
00:08:28,820 --> 00:08:32,460
because that's the ratio
of the labels in the data.

222
00:08:32,460 --> 00:08:34,640
Now we'll continue by
building the false branch.

223
00:08:34,640 --> 00:08:36,860
And here, this will
also become a leaf.

224
00:08:36,860 --> 00:08:40,590
We'll predict apple
with 100% confidence.

225
00:08:40,590 --> 00:08:42,480
Now the previous call
returns, and this node

226
00:08:42,480 --> 00:08:44,986
becomes a decision node.

227
00:08:44,986 --> 00:08:46,860
In code, that just means
it holds a reference

228
00:08:46,860 --> 00:08:49,975
to the question we asked and
the two child nodes that result.

229
00:08:49,975 --> 00:08:52,020
And we're nearly done.

230
00:08:52,020 --> 00:08:55,230
Now we return to the root node
and build the false branch.

231
00:08:55,230 --> 00:08:58,200
There are no further questions
to ask, so this becomes a leaf.

232
00:08:58,200 --> 00:09:00,860
And that predicts grape
with 100% confidence.

233
00:09:00,860 --> 00:09:04,050
And finally, the root node
also becomes a decision node.

234
00:09:04,050 --> 00:09:06,786
And our call to build tree
returns a reference to it.

235
00:09:06,787 --> 00:09:08,370
If you scroll down
in the code, you'll

236
00:09:08,370 --> 00:09:10,740
see that I've added functions
to classify data and print

237
00:09:10,740 --> 00:09:11,490
the tree.

238
00:09:11,490 --> 00:09:13,594
And these start with a
reference to the root node,

239
00:09:13,594 --> 00:09:14,760
so you can see how it works.

240
00:09:14,760 --> 00:09:17,610


241
00:09:17,610 --> 00:09:18,850
OK, hope that was helpful.

242
00:09:18,850 --> 00:09:21,340
And you can check out the
code for more details.

243
00:09:21,340 --> 00:09:23,680
There's a lot more I have
to say about decision trees,

244
00:09:23,680 --> 00:09:25,954
but there's only so much we
can fit into a short time.

245
00:09:25,955 --> 00:09:28,330
Here are a couple of topics
that are good to be aware of.

246
00:09:28,330 --> 00:09:30,413
And you can check out the
books in the description

247
00:09:30,413 --> 00:09:31,420
to learn more.

248
00:09:31,420 --> 00:09:33,430
As a next step, I'd
recommend modifying the tree

249
00:09:33,430 --> 00:09:35,223
to work with your own data set.

250
00:09:35,224 --> 00:09:36,640
And this can be a
fun way to build

251
00:09:36,640 --> 00:09:38,830
a simple and interpretable
classifier for use

252
00:09:38,830 --> 00:09:40,011
in your projects.

253
00:09:40,011 --> 00:09:41,260
Thanks for watching, everyone.

254
00:09:41,260 --> 00:09:43,290
And I'll see you next time.

255
00:09:43,290 --> 00:09:52,038


