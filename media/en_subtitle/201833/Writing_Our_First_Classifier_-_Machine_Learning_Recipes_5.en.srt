1
00:00:00,000 --> 00:00:06,030
[MUSIC PLAYING]

2
00:00:06,030 --> 00:00:06,710
Hey, everyone.

3
00:00:06,710 --> 00:00:07,910
Welcome back.

4
00:00:07,910 --> 00:00:10,220
In this episode, we're going
to do something special,

5
00:00:10,220 --> 00:00:13,164
and that's write our own
classifier from scratch.

6
00:00:13,164 --> 00:00:14,580
If you're new to
machine learning,

7
00:00:14,580 --> 00:00:16,170
this is a big milestone.

8
00:00:16,170 --> 00:00:18,560
Because if you can follow
along and do this on your own,

9
00:00:18,560 --> 00:00:21,890
it means you understand an
important piece of the puzzle.

10
00:00:21,890 --> 00:00:23,670
The classifier we're
going to write today

11
00:00:23,670 --> 00:00:26,390
is a scrappy version
of k-Nearest Neighbors.

12
00:00:26,390 --> 00:00:29,660
That's one of the simplest
classifiers around.

13
00:00:29,660 --> 00:00:32,860
First, here's a quick outline of
what we'll do in this episode.

14
00:00:32,860 --> 00:00:35,170
We'll start with our code
from Episode 4, Let's

15
00:00:35,170 --> 00:00:36,570
Write a Pipeline.

16
00:00:36,570 --> 00:00:39,290
Recall in that episode we
did a simple experiment.

17
00:00:39,290 --> 00:00:42,720
We imported a data set and
split it into train and test.

18
00:00:42,720 --> 00:00:44,580
We used train to
train a classifier,

19
00:00:44,580 --> 00:00:47,150
and test to see how
accurate it was.

20
00:00:47,150 --> 00:00:48,760
Writing the
classifier is the part

21
00:00:48,760 --> 00:00:50,940
we're going to focus on today.

22
00:00:50,940 --> 00:00:52,739
Previously we imported
the classifier

23
00:00:52,740 --> 00:00:55,280
from a library using
these two lines.

24
00:00:55,280 --> 00:00:58,190
Here we'll comment them
out and write our own.

25
00:00:58,190 --> 00:01:01,539
The rest of the pipeline
will stay exactly the same.

26
00:01:01,539 --> 00:01:03,830
I'll pop in and out of the
screencast to explain things

27
00:01:03,830 --> 00:01:05,940
as we go along.

28
00:01:05,940 --> 00:01:08,530
To start, let's run our
pipeline to remind ourselves

29
00:01:08,530 --> 00:01:10,120
what the accuracy was.

30
00:01:10,120 --> 00:01:12,370
As you can see, it's over 90%.

31
00:01:12,370 --> 00:01:14,260
And that's the goal
for the classifier

32
00:01:14,260 --> 00:01:15,660
we'll write ourselves.

33
00:01:15,660 --> 00:01:17,679
Now let's comment
out that import.

34
00:01:17,680 --> 00:01:19,540
Right off the bat,
this breaks our code.

35
00:01:19,540 --> 00:01:22,250
So the first thing we need
to do is fix our pipeline.

36
00:01:22,250 --> 00:01:24,850
And to do that, we'll implement
a class for our classifier.

37
00:01:24,850 --> 00:01:27,690


38
00:01:27,690 --> 00:01:29,580
I'll call it ScrappyKNN.

39
00:01:29,580 --> 00:01:31,690
And by scrappy, I
mean bare bones.

40
00:01:31,690 --> 00:01:33,710
Just enough to get it working.

41
00:01:33,710 --> 00:01:38,110
Next, I'll change our
pipeline to use it.

42
00:01:38,110 --> 00:01:40,880
Now let's see what methods
we need to implement.

43
00:01:40,880 --> 00:01:42,949
Looking at the interface
for a classifier,

44
00:01:42,950 --> 00:01:45,500
we see there are two
we care about-- fit,

45
00:01:45,500 --> 00:01:47,470
which does the
training, and predict,

46
00:01:47,470 --> 00:01:49,270
which does the prediction.

47
00:01:49,270 --> 00:01:51,600
First we'll declare
our fit method.

48
00:01:51,600 --> 00:01:54,440
Remember this takes the features
and labels for the training set

49
00:01:54,440 --> 00:01:57,679
as input, so we'll add
parameters for those.

50
00:01:57,680 --> 00:02:00,090
Now let's move on to
our predict method.

51
00:02:00,090 --> 00:02:03,470
As input, this receives the
features for our testing data.

52
00:02:03,470 --> 00:02:06,920
And as output, it returns
predictions for the labels.

53
00:02:06,920 --> 00:02:09,389
Our first goal is to get
the pipeline working,

54
00:02:09,389 --> 00:02:12,230
and to understand
what these methods do.

55
00:02:12,230 --> 00:02:14,179
So before we write
our real classifier,

56
00:02:14,180 --> 00:02:15,810
we'll start with
something simpler.

57
00:02:15,810 --> 00:02:18,240
We'll write a random classifier.

58
00:02:18,240 --> 00:02:21,690
And by random, I mean
we'll just guess the label.

59
00:02:21,690 --> 00:02:25,310
To start, we'll add some code
to the fit and predict methods.

60
00:02:25,310 --> 00:02:28,460
In fit, I'll store the
training data in this class.

61
00:02:28,460 --> 00:02:30,480
You can think of this
as just memorizing it.

62
00:02:30,480 --> 00:02:32,840
And you'll see why
we do that later on.

63
00:02:32,840 --> 00:02:34,670
Inside the predict
method, remember

64
00:02:34,670 --> 00:02:37,410
that we'll need to return
a list of predictions.

65
00:02:37,410 --> 00:02:40,090
That's because the parameter,
X_test, is actually

66
00:02:40,090 --> 00:02:42,730
a 2D array, or list of lists.

67
00:02:42,730 --> 00:02:46,410
Each row contains the features
for one testing example.

68
00:02:46,410 --> 00:02:48,170
To make a prediction
for each row,

69
00:02:48,170 --> 00:02:50,829
I'll just randomly pick a
label from the training data

70
00:02:50,830 --> 00:02:53,340
and append that to
our predictions.

71
00:02:53,340 --> 00:02:55,660
At this point, our
pipeline is working again.

72
00:02:55,660 --> 00:02:58,029
So let's run it and
see how well it does.

73
00:02:58,029 --> 00:03:00,070
Recall there are three
different types of flowers

74
00:03:00,070 --> 00:03:05,070
in the iris dataset, so
accuracy should be about 33%.

75
00:03:05,070 --> 00:03:07,109
Now we know the interface
for a classifier.

76
00:03:07,110 --> 00:03:09,060
But when we started
this exercise,

77
00:03:09,060 --> 00:03:11,770
our accuracy was above 90%.

78
00:03:11,770 --> 00:03:14,500
So let's see if
we can do better.

79
00:03:14,500 --> 00:03:16,850
To do that, we'll
implement our classifier,

80
00:03:16,850 --> 00:03:19,380
which is based on
k-Nearest Neighbors.

81
00:03:19,380 --> 00:03:22,469
Here's the intuition for
how that algorithm works.

82
00:03:22,469 --> 00:03:24,760
Let's return to our drawings
of green dots and red dots

83
00:03:24,760 --> 00:03:26,399
from the last episode.

84
00:03:26,400 --> 00:03:27,990
Imagine the dots we
see on the screen

85
00:03:27,990 --> 00:03:30,880
are the training data we
memorized in the fit method,

86
00:03:30,880 --> 00:03:33,420
say for a toy dataset.

87
00:03:33,420 --> 00:03:36,220
Now imagine we're asked to make
a prediction for this testing

88
00:03:36,220 --> 00:03:38,260
point that I'll
draw here in gray.

89
00:03:38,260 --> 00:03:39,959
How can we do that?

90
00:03:39,960 --> 00:03:41,890
Well in a nearest
neighbor classifier,

91
00:03:41,890 --> 00:03:44,220
it works exactly like it sounds.

92
00:03:44,220 --> 00:03:45,940
We'll find the
training point that's

93
00:03:45,940 --> 00:03:48,070
closest to the testing point.

94
00:03:48,070 --> 00:03:50,392
This point is the
nearest neighbor.

95
00:03:50,392 --> 00:03:51,850
Then we'll predict
that the testing

96
00:03:51,850 --> 00:03:54,170
point has the same label.

97
00:03:54,170 --> 00:03:56,880
For example, we'll guess that
this testing dot is green,

98
00:03:56,880 --> 00:03:59,880
because that's the color
of its nearest neighbor.

99
00:03:59,880 --> 00:04:02,430
As another example, if we
had a testing dot over here,

100
00:04:02,430 --> 00:04:04,170
we'd guess that it's red.

101
00:04:04,170 --> 00:04:06,399
Now what about this one
right in the middle?

102
00:04:06,400 --> 00:04:08,730
Imagine that this dot is
equidistant to the nearest

103
00:04:08,730 --> 00:04:10,750
green dot and the
nearest red one.

104
00:04:10,750 --> 00:04:13,570
There's a tie, so how
could we classify it?

105
00:04:13,570 --> 00:04:15,959
Well one way is we could
randomly break the tie.

106
00:04:15,960 --> 00:04:18,970
But there's another way,
and that's where k comes in.

107
00:04:18,970 --> 00:04:20,680
K is the number of
neighbors we consider

108
00:04:20,680 --> 00:04:22,340
when making our prediction.

109
00:04:22,340 --> 00:04:25,530
If k was 1, we'd just look at
the closest training point.

110
00:04:25,530 --> 00:04:28,169
But if k was 3, we'd look
at the three closest.

111
00:04:28,170 --> 00:04:30,860
In this case, two of those
are green and one is red.

112
00:04:30,860 --> 00:04:34,410
To predict, we could vote and
predict the majority class.

113
00:04:34,410 --> 00:04:36,230
Now there's more detail
to this algorithm,

114
00:04:36,230 --> 00:04:38,540
but that's enough
to get us started.

115
00:04:38,540 --> 00:04:40,200
To code this up,
first we'll need a way

116
00:04:40,200 --> 00:04:42,700
to find the nearest neighbor.

117
00:04:42,700 --> 00:04:44,840
And to do that, we'll
measure the straight line

118
00:04:44,840 --> 00:04:48,950
distance between two points,
just like you do with a ruler.

119
00:04:48,950 --> 00:04:52,099
There's a formula for that
called the Euclidean Distance,

120
00:04:52,100 --> 00:04:54,310
and here's what the
formula looks like.

121
00:04:54,310 --> 00:04:56,590
It measures the distance
between two points,

122
00:04:56,590 --> 00:04:59,250
and it works a bit like
the Pythagorean Theorem.

123
00:04:59,250 --> 00:05:02,350
A squared plus B squared
equals C squared.

124
00:05:02,350 --> 00:05:04,790
You can think of this term
as A, or the difference

125
00:05:04,790 --> 00:05:06,840
between the first two features.

126
00:05:06,840 --> 00:05:08,669
Likewise, you can think
of this term as B,

127
00:05:08,670 --> 00:05:11,170
or the difference between
the second pair of features.

128
00:05:11,170 --> 00:05:14,740
And the distance we compute is
the length of the hypotenuse.

129
00:05:14,740 --> 00:05:16,460
Now here's something cool.

130
00:05:16,460 --> 00:05:17,940
Right now we're
computing distance

131
00:05:17,940 --> 00:05:20,670
in two-dimensional space,
because we have just two

132
00:05:20,670 --> 00:05:22,780
features in our toy dataset.

133
00:05:22,780 --> 00:05:25,969
But what if we had three
features or three dimensions?

134
00:05:25,970 --> 00:05:28,200
Well then we'd be in a cube.

135
00:05:28,200 --> 00:05:30,450
We can still visualize
how to measure distance

136
00:05:30,450 --> 00:05:32,500
in the space with a ruler.

137
00:05:32,500 --> 00:05:35,270
But what if we had four
features or four dimensions,

138
00:05:35,270 --> 00:05:36,710
like we do in iris?

139
00:05:36,710 --> 00:05:38,500
Well, now we're in
a hypercube, and we

140
00:05:38,500 --> 00:05:40,740
can't visualize this very easy.

141
00:05:40,740 --> 00:05:42,450
The good news is the
Euclidean Distance

142
00:05:42,450 --> 00:05:46,010
works the same way regardless
of the number of dimensions.

143
00:05:46,010 --> 00:05:50,050
With more features, we can just
add more terms to the equation.

144
00:05:50,050 --> 00:05:52,060
You can find more details
about this online.

145
00:05:52,060 --> 00:05:54,670


146
00:05:54,670 --> 00:05:56,770
Now let's code up
Euclidean distance.

147
00:05:56,770 --> 00:05:58,270
There are plenty
of ways to do that,

148
00:05:58,270 --> 00:06:00,370
but we'll use a library
called scipy that's

149
00:06:00,370 --> 00:06:02,460
already installed by Anaconda.

150
00:06:02,460 --> 00:06:05,380
Here, A and B are lists
of numeric features.

151
00:06:05,380 --> 00:06:07,330
Say A is a point from
our training data,

152
00:06:07,330 --> 00:06:09,800
and B is a point from
our testing data.

153
00:06:09,800 --> 00:06:12,860
This function returns the
distance between them.

154
00:06:12,860 --> 00:06:14,440
That's all the math
we need, so now

155
00:06:14,440 --> 00:06:17,390
let's take a look at the
algorithm for a classifier.

156
00:06:17,390 --> 00:06:19,479
To make a prediction
for a test point,

157
00:06:19,480 --> 00:06:22,250
we'll calculate the distance
to all the training points.

158
00:06:22,250 --> 00:06:25,030
Then we'll predict the testing
point has the same label

159
00:06:25,030 --> 00:06:27,140
as the closest one.

160
00:06:27,140 --> 00:06:28,909
I'll delete the random
prediction we made,

161
00:06:28,910 --> 00:06:31,610
and replace it with a method
that finds the closest training

162
00:06:31,610 --> 00:06:33,470
point to the test point.

163
00:06:33,470 --> 00:06:35,580
For this video hard,
I'll hard-code k to 1,

164
00:06:35,580 --> 00:06:38,090
so we're writing a nearest
neighbor classifier.

165
00:06:38,090 --> 00:06:40,099
The k variable won't
appear in our code,

166
00:06:40,100 --> 00:06:42,950
since we'll always just
find the closest point.

167
00:06:42,950 --> 00:06:45,700
Inside this method, we'll loop
over all the training points

168
00:06:45,700 --> 00:06:48,250
and keep track of the
closest one so far.

169
00:06:48,250 --> 00:06:50,650
Remember that we memorized
the training data in our fit

170
00:06:50,650 --> 00:06:54,190
function, and X_train
contains the features.

171
00:06:54,190 --> 00:06:56,910
To start, I'll calculate the
distance from the test point

172
00:06:56,910 --> 00:06:58,740
to the first training point.

173
00:06:58,740 --> 00:07:00,990
I'll use this variable to
keep track of the shortest

174
00:07:00,990 --> 00:07:02,584
distance we've found so far.

175
00:07:02,584 --> 00:07:04,000
And I'll use this
variable to keep

176
00:07:04,000 --> 00:07:07,400
track of the index of the
training point that's closest.

177
00:07:07,400 --> 00:07:09,909
We'll need this later
to retrieve its label.

178
00:07:09,910 --> 00:07:12,370
Now we'll iterate over all
the other training points.

179
00:07:12,370 --> 00:07:14,410
And every time we
find a closer one,

180
00:07:14,410 --> 00:07:16,150
we'll update our variables.

181
00:07:16,150 --> 00:07:18,099
Finally, we'll use
the index to return

182
00:07:18,100 --> 00:07:22,110
the label for the
closest training example.

183
00:07:22,110 --> 00:07:24,780
At this point, we have a working
nearest neighbor classifier,

184
00:07:24,780 --> 00:07:29,510
so let's run it and see
what the accuracy is.

185
00:07:29,510 --> 00:07:31,330
As you can see, it's over 90%.

186
00:07:31,330 --> 00:07:32,250
And we did it.

187
00:07:32,250 --> 00:07:34,240
When you run this on
your own, the accuracy

188
00:07:34,240 --> 00:07:36,906
might be a bit different because
of randomness in the train test

189
00:07:36,906 --> 00:07:38,530
split.

190
00:07:38,530 --> 00:07:40,739
Now if you can code this
up and understand it,

191
00:07:40,740 --> 00:07:42,670
that's a big
accomplishment because it

192
00:07:42,670 --> 00:07:46,254
means you can write a simple
classifier from scratch.

193
00:07:46,254 --> 00:07:47,920
Now, there are a
number of pros and cons

194
00:07:47,920 --> 00:07:50,850
to this algorithm, many of
which you can find online.

195
00:07:50,850 --> 00:07:53,580
The basic pro is that it's
relatively easy to understand,

196
00:07:53,580 --> 00:07:56,000
and works reasonably
well for some problems.

197
00:07:56,000 --> 00:07:57,670
And the basic cons
are that it's slow,

198
00:07:57,670 --> 00:07:59,990
because it has to iterate
over every training point

199
00:07:59,990 --> 00:08:01,550
to make a prediction.

200
00:08:01,550 --> 00:08:04,070
And importantly, as
we saw in Episode 3,

201
00:08:04,070 --> 00:08:06,384
some features are more
informative than others.

202
00:08:06,384 --> 00:08:08,050
But there's not an
easy way to represent

203
00:08:08,050 --> 00:08:10,120
that in k-Nearest Neighbors.

204
00:08:10,120 --> 00:08:12,150
In the long run, we
want a classifier

205
00:08:12,150 --> 00:08:15,260
that learns more complex
relationships between features

206
00:08:15,260 --> 00:08:17,289
and the label we're
trying to predict.

207
00:08:17,290 --> 00:08:19,490
A decision tree is a
good example of that.

208
00:08:19,490 --> 00:08:22,120
And a neural network like we
saw in TensorFlow Playground

209
00:08:22,120 --> 00:08:23,730
is even better.

210
00:08:23,730 --> 00:08:24,940
OK, hope that was helpful.

211
00:08:24,940 --> 00:08:26,227
Thanks as always for watching.

212
00:08:26,227 --> 00:08:28,560
You can follow me on Twitter
for updates and, of course,

213
00:08:28,560 --> 00:08:29,310
Google Developers.

214
00:08:29,310 --> 00:08:32,190
And I'll see you guys next time.

215
00:08:32,190 --> 00:08:35,540
[MUSIC PLAYING]

216
00:08:35,539 --> 00:08:43,678


