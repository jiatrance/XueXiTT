1
00:00:00,000 --> 00:00:03,360
[MUSIC PLAYING]

2
00:00:03,360 --> 00:00:05,814


3
00:00:05,815 --> 00:00:06,940
JOSH GORDON: Hey, everyone.

4
00:00:06,940 --> 00:00:08,129
Welcome back.

5
00:00:08,129 --> 00:00:10,170
Features are the way you
represent your knowledge

6
00:00:10,170 --> 00:00:12,320
about the world
for the classifier,

7
00:00:12,320 --> 00:00:14,130
and today I'll walk
you through techniques

8
00:00:14,130 --> 00:00:16,740
you can use to represent
your features and utilities

9
00:00:16,740 --> 00:00:19,169
TensorFlow provides to help.

10
00:00:19,170 --> 00:00:22,320
You use a dataset from the
US census as an example,

11
00:00:22,320 --> 00:00:24,480
and the goal is to predict
if someone's income is

12
00:00:24,480 --> 00:00:27,960
greater than $50,000 based
on attributes like their age

13
00:00:27,960 --> 00:00:29,760
and occupation.

14
00:00:29,760 --> 00:00:32,310
The dataset is
stored as a CSV file,

15
00:00:32,310 --> 00:00:35,580
and previously we've seen how to
use the column values directly

16
00:00:35,580 --> 00:00:37,230
as features.

17
00:00:37,230 --> 00:00:38,879
But today we'll use
feature engineering

18
00:00:38,880 --> 00:00:42,360
to transform them into a
more useful representation.

19
00:00:42,360 --> 00:00:45,210
As we go, I'll visualize
what these transformations do

20
00:00:45,210 --> 00:00:48,180
using a tool called Facets,
and you can find a link to it

21
00:00:48,180 --> 00:00:49,800
in the description.

22
00:00:49,800 --> 00:00:52,739
You'll also find complete code
to train a TensorFlow estimator

23
00:00:52,740 --> 00:00:54,420
on this dataset.

24
00:00:54,420 --> 00:00:55,410
OK, let's get started.

25
00:00:55,410 --> 00:00:57,959


26
00:00:57,960 --> 00:01:00,230
Let's begin with a numeric
attribute like age,

27
00:01:00,230 --> 00:01:03,230
and think about how we can
use it to predict income.

28
00:01:03,230 --> 00:01:05,840
Now if you think about how
age correlates with income,

29
00:01:05,840 --> 00:01:08,300
our first intuition is
that as age increases,

30
00:01:08,300 --> 00:01:10,399
usually so does income.

31
00:01:10,400 --> 00:01:12,320
And the simplest way
to represent this

32
00:01:12,320 --> 00:01:14,509
would just be to take
the raw numeric value

33
00:01:14,510 --> 00:01:16,197
and use that as a feature.

34
00:01:16,197 --> 00:01:17,780
Here we're building
a list of features

35
00:01:17,780 --> 00:01:20,360
we use to train the
model, and each of these

36
00:01:20,360 --> 00:01:22,340
is stored as a feature column.

37
00:01:22,340 --> 00:01:25,340
This contains data about
the column from the CSV file

38
00:01:25,340 --> 00:01:27,160
and how to represent it.

39
00:01:27,160 --> 00:01:30,619
Here we'll write a feature that
just uses the raw value of age,

40
00:01:30,620 --> 00:01:34,100
and this string corresponds
to a column in the CSV file.

41
00:01:34,100 --> 00:01:36,442
Now what can go wrong
with this approach?

42
00:01:36,442 --> 00:01:38,149
Well, if we think more
closely about age,

43
00:01:38,150 --> 00:01:41,630
we realize it's not in a linear
relationship with income.

44
00:01:41,630 --> 00:01:43,979
The curve might look
something like this.

45
00:01:43,980 --> 00:01:47,000
It's flat for children, then
increases during working age,

46
00:01:47,000 --> 00:01:49,220
and decreases during retirement.

47
00:01:49,220 --> 00:01:50,810
A linear classifier,
for example,

48
00:01:50,810 --> 00:01:53,480
is unable to capture
this relationship.

49
00:01:53,480 --> 00:01:55,940
That's because it learns a
single weight for each feature.

50
00:01:55,940 --> 00:01:58,820


51
00:01:58,820 --> 00:02:01,550
To make it easier for the
classifier, one thing we can do

52
00:02:01,550 --> 00:02:03,200
is bucket the feature.

53
00:02:03,200 --> 00:02:05,510
And bucketing transforms
a numeric feature

54
00:02:05,510 --> 00:02:07,700
into several
categorical ones based

55
00:02:07,700 --> 00:02:11,150
on the range it falls into,
and each of these new features

56
00:02:11,150 --> 00:02:14,940
indicate whether a person's
age falls into that range.

57
00:02:14,940 --> 00:02:17,480
And now a linear model can
capture the relationship

58
00:02:17,480 --> 00:02:20,079
by learning different
weights for each bucket.

59
00:02:20,080 --> 00:02:22,430
Let's see how this
looks in Facets.

60
00:02:22,430 --> 00:02:23,870
Conveniently,
there's a live demo

61
00:02:23,870 --> 00:02:27,350
that runs in the browser with
our census data preloaded,

62
00:02:27,350 --> 00:02:30,200
and each individual from
the CSV is visualized

63
00:02:30,200 --> 00:02:32,720
as a dot colored by income.

64
00:02:32,720 --> 00:02:36,550
If you click on a dot, you can
see stats about the person.

65
00:02:36,550 --> 00:02:38,690
Now let's bucket
by age, and you can

66
00:02:38,690 --> 00:02:41,900
adjust the number of buckets to
make it more or less granular.

67
00:02:41,900 --> 00:02:44,240
How you choose the number
of buckets is up to you,

68
00:02:44,240 --> 00:02:46,760
and ideally, you'd want to use
your knowledge of the problem

69
00:02:46,760 --> 00:02:48,560
to do this well.

70
00:02:48,560 --> 00:02:50,840
In TensorFlow, we can
create a bucketized feature

71
00:02:50,840 --> 00:02:53,720
by wrapping a numeric
column from the CSV.

72
00:02:53,720 --> 00:02:55,340
And here we're
specifying the number

73
00:02:55,340 --> 00:02:58,220
and the ranges of the
buckets we'd like created.

74
00:02:58,220 --> 00:03:00,440
Once this is done, we can
add the bucketized feature

75
00:03:00,440 --> 00:03:02,030
to the list used
to train our model.

76
00:03:02,030 --> 00:03:05,200


77
00:03:05,200 --> 00:03:07,799
Now let's see how to represent
a categorical feature,

78
00:03:07,800 --> 00:03:10,740
and I'll use the education
column as an example.

79
00:03:10,740 --> 00:03:12,600
Because there are
only a few values,

80
00:03:12,600 --> 00:03:16,207
the best way to represent this
is just use the raw value.

81
00:03:16,207 --> 00:03:17,790
And here we'll create
a feature column

82
00:03:17,790 --> 00:03:21,420
that says education can be a
single value from this list.

83
00:03:21,420 --> 00:03:24,359
Of course, you could also read
the values from a file on disk

84
00:03:24,360 --> 00:03:27,030
rather than writing
them out in code.

85
00:03:27,030 --> 00:03:28,950
Now using the raw value
is the right thing

86
00:03:28,950 --> 00:03:32,164
to do when there are only a
small number of possibilities.

87
00:03:32,164 --> 00:03:34,079
We'll cover the case
where there are thousands

88
00:03:34,080 --> 00:03:35,716
of possibilities in a moment.

89
00:03:35,716 --> 00:03:37,590
First, let's take a look
at feature crossing.

90
00:03:37,590 --> 00:03:40,372


91
00:03:40,372 --> 00:03:42,580
Feature crossing is a way
to create new features that

92
00:03:42,580 --> 00:03:44,770
are combinations
of existing ones,

93
00:03:44,770 --> 00:03:47,780
and these can be especially
helpful to linear classifiers,

94
00:03:47,780 --> 00:03:51,040
which can't model
interactions between features.

95
00:03:51,040 --> 00:03:53,410
Here's what this
looks like in Facets.

96
00:03:53,410 --> 00:03:55,150
I'll take our age
buckets from before

97
00:03:55,150 --> 00:03:57,400
and cross them with education.

98
00:03:57,400 --> 00:04:00,070
Under the hood, you can think
of a true-false feature being

99
00:04:00,070 --> 00:04:02,079
created for each
bucket that tells

100
00:04:02,080 --> 00:04:04,060
the classifier whether
an individual falls

101
00:04:04,060 --> 00:04:05,890
into that range.

102
00:04:05,890 --> 00:04:07,690
Now these buckets
can be informative,

103
00:04:07,690 --> 00:04:10,660
and here we see some groups are
likely to have a high income,

104
00:04:10,660 --> 00:04:12,220
and others low.

105
00:04:12,220 --> 00:04:15,850
In code, using a feature cross
works the same way as before.

106
00:04:15,850 --> 00:04:17,860
We'll cross our age
buckets with education

107
00:04:17,860 --> 00:04:20,300
and add it to the list
of features to use.

108
00:04:20,300 --> 00:04:23,345
A feature cross can generate
many possibilities quickly,

109
00:04:23,345 --> 00:04:24,970
which is why they
are often represented

110
00:04:24,970 --> 00:04:26,140
under the hood with a hash.

111
00:04:26,140 --> 00:04:29,044


112
00:04:29,044 --> 00:04:32,440
A hashed feature column is one
way to efficiently represent

113
00:04:32,440 --> 00:04:35,380
a categorical feature
with a large vocabulary.

114
00:04:35,380 --> 00:04:37,000
More importantly,
you can use these

115
00:04:37,000 --> 00:04:38,440
as a way to make
your data easier

116
00:04:38,440 --> 00:04:40,750
to work with because
they free you from having

117
00:04:40,750 --> 00:04:43,300
to provide a vocabulary list.

118
00:04:43,300 --> 00:04:45,760
In this example, we'll
represent the occupation column

119
00:04:45,760 --> 00:04:48,190
from our CSV file
by using a hash

120
00:04:48,190 --> 00:04:50,570
with 1,000 possible values.

121
00:04:50,570 --> 00:04:53,380
Notice we don't have to
provide a vocabulary list,

122
00:04:53,380 --> 00:04:55,570
and to avoid collisions,
I've set the hash size

123
00:04:55,570 --> 00:04:58,840
so it's larger than the number
of items in the vocabulary.

124
00:04:58,840 --> 00:05:01,060
Here's how this
works under the hood.

125
00:05:01,060 --> 00:05:03,130
Normally, a categorical
feature is represented

126
00:05:03,130 --> 00:05:04,840
as a one hot encoding.

127
00:05:04,840 --> 00:05:07,239
That means there's one bit
for each possible value

128
00:05:07,240 --> 00:05:08,620
in the vocabulary.

129
00:05:08,620 --> 00:05:11,110
And we can create a lookup
because we know the vocabulary

130
00:05:11,110 --> 00:05:12,790
list in advance.

131
00:05:12,790 --> 00:05:14,500
Now if we don't
know the vocab, we

132
00:05:14,500 --> 00:05:17,680
can use a hash function to
compute the bit automatically.

133
00:05:17,680 --> 00:05:19,810
The downside is there
could be collisions,

134
00:05:19,810 --> 00:05:23,020
meaning different items are
mapped to the same value.

135
00:05:23,020 --> 00:05:25,630
Hashes can also be used
to limit memory usage

136
00:05:25,630 --> 00:05:28,540
at the cost of adding some
noise to your training data.

137
00:05:28,540 --> 00:05:30,247
If you have a large
vocabulary, it

138
00:05:30,247 --> 00:05:32,080
can be memory intensive
to use that as input

139
00:05:32,080 --> 00:05:33,325
to a neural network.

140
00:05:33,325 --> 00:05:35,380
A hashed column can
be used to limit

141
00:05:35,380 --> 00:05:37,540
the maximum number
of possibilities,

142
00:05:37,540 --> 00:05:39,160
but I prefer them
simply as a tool

143
00:05:39,160 --> 00:05:40,450
to save you programming time.

144
00:05:40,450 --> 00:05:43,370


145
00:05:43,370 --> 00:05:45,290
Finally, I'd like to
mention embeddings,

146
00:05:45,290 --> 00:05:47,780
and these can be less intuitive
than the other techniques,

147
00:05:47,780 --> 00:05:50,113
but they're a powerful way
to work with categorical data

148
00:05:50,113 --> 00:05:51,950
in a deep learning setting.

149
00:05:51,950 --> 00:05:54,620
You can think of an embedding
as a vector that represents

150
00:05:54,620 --> 00:05:56,270
the meaning of a word.

151
00:05:56,270 --> 00:05:58,490
And we can visualize a
dataset of word embeddings

152
00:05:58,490 --> 00:06:00,830
using the TensorFlow
Embedding Projector,

153
00:06:00,830 --> 00:06:04,070
and there's an online demo you
can find in the description.

154
00:06:04,070 --> 00:06:07,340
Here we're looking at a dataset
of 10,000 words, each of which

155
00:06:07,340 --> 00:06:09,830
is represented by a vector
with many dimensions,

156
00:06:09,830 --> 00:06:12,440
projected down to 3D
so we can see them.

157
00:06:12,440 --> 00:06:14,612
You can search for words
in the box to the right.

158
00:06:14,612 --> 00:06:16,070
And if you experiment
a bit, you'll

159
00:06:16,070 --> 00:06:18,650
find similar words are
often close together.

160
00:06:18,650 --> 00:06:21,827
For example, all of the words
in this cluster are cities.

161
00:06:21,827 --> 00:06:23,660
What's neat about
embeddings is that they're

162
00:06:23,660 --> 00:06:26,900
learned automatically in the
process of training a DNN.

163
00:06:26,900 --> 00:06:28,700
And to make that happen,
all you need to do

164
00:06:28,700 --> 00:06:30,587
is write an embedding column.

165
00:06:30,587 --> 00:06:32,420
Here we'll create an
embedding for education

166
00:06:32,420 --> 00:06:34,852
with 10 dimensions.

167
00:06:34,852 --> 00:06:37,310
Now embeddings are helpful if
you have a categorical column

168
00:06:37,310 --> 00:06:39,140
with a large
vocabulary and you want

169
00:06:39,140 --> 00:06:42,020
to compress the representation
so the classifier learns

170
00:06:42,020 --> 00:06:44,419
general concepts
rather than memorizing

171
00:06:44,420 --> 00:06:46,850
the meaning of specific words.

172
00:06:46,850 --> 00:06:48,620
For example, imagine
if the census data

173
00:06:48,620 --> 00:06:50,840
had a column called job title.

174
00:06:50,840 --> 00:06:52,710
There are thousands
of different jobs,

175
00:06:52,710 --> 00:06:55,310
and an embedding could be used
to help your classifier learn

176
00:06:55,310 --> 00:06:57,980
that words like programmer
and software engineer

177
00:06:57,980 --> 00:06:59,150
often mean the same thing.

178
00:06:59,150 --> 00:07:01,929


179
00:07:01,930 --> 00:07:04,090
OK, hope this was
a helpful intro,

180
00:07:04,090 --> 00:07:06,210
and thinking about how to
represent your features

181
00:07:06,210 --> 00:07:07,960
is one of the most
important contributions

182
00:07:07,960 --> 00:07:10,397
you can make to a machine
learning experiment.

183
00:07:10,397 --> 00:07:11,980
Feature columns are
great because they

184
00:07:11,980 --> 00:07:14,021
let you experiment with
different representations

185
00:07:14,021 --> 00:07:16,260
in code and make advanced
features like embeddings

186
00:07:16,260 --> 00:07:17,590
accessible.

187
00:07:17,590 --> 00:07:19,090
As a next step,
I'd recommend you

188
00:07:19,090 --> 00:07:21,940
try the code in the description
and see if you can modify it

189
00:07:21,940 --> 00:07:23,950
for a problem you care about.

190
00:07:23,950 --> 00:07:26,770
Thanks for watching everyone,
and I'll see you next time.

191
00:07:26,770 --> 00:07:30,120
[MUSIC PLAYING]

192
00:07:30,120 --> 00:07:37,408


